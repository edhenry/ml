{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "\n",
    "# Make sure you've got Numpy and Scipy installed:\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial.distance\n",
    "from numpy.linalg import svd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import fileinput\n",
    "from glob import glob\n",
    "\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Matrices\n",
    "\n",
    "We can start by creating a dictionary $d$ that can be used to map word-pairs to counts. Everytime a pair of words $w$ and $w^\\prime$ occur, we can increment a counter associated with each pair $d[w,w^\\prime]$ by $1$. Using these count dictionary we can then create our vocabulary $V$, an ordered list of words types.\n",
    "\n",
    "We can then create a matrix, $M$, of dimensions $|V|$ x $|V|$. Each $M[i,j]$ is filled with the counts contained in $d[w_i,w_j]$.\n",
    "\n",
    "These co-occurence matrices have been provided with this example. We can import them with the _build_ function seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build method to import co-occurence matrices\n",
    "\n",
    "def build(src_filename, delimiter=',', header=True, quoting=csv.QUOTE_MINIMAL):    \n",
    "    reader = csv.reader(open(src_filename), delimiter=delimiter, quoting=quoting)\n",
    "    colnames = None\n",
    "    if header:\n",
    "        colnames = reader.__next__()\n",
    "        colnames = colnames[1: ]\n",
    "    mat = []    \n",
    "    rownames = []\n",
    "    for line in reader:\n",
    "        rownames.append(line[0])            \n",
    "        mat.append(np.array(list(map(float, line[1: ]))))    \n",
    "    return (np.array(mat), rownames, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now read in the example co-occurency matrices for later. We will be using the IMDB moview review dataset for these examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import word <-> word co-occurence matrix\n",
    "ww = build('distributedwordreps-data/imdb-wordword.csv')\n",
    "\n",
    "#Import w <-> document co-occurence matrix\n",
    "wd = build('distributedwordreps-data/imdb-worddoc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ww[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example of counts of first words in the document which happen to be two exclamation marks\n",
    "\n",
    "print(\"Word co-occurences : \" + ww[1][0] + \", and, \" + ww[2][0])\n",
    "\n",
    "print(\"Count : \" + str(ww[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load GloVe vectors as well\n",
    "\n",
    "glv = build('distributedwordreps-data/glove.6B.50d.txt', delimiter=' ', header=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Comparisons\n",
    "\n",
    "For the most part we are interested in measuring the _distance_ between two vectors. The general idea of vector comparisons using distance is that words that are semantically similar should be closer together in the vector spaces we build, and symantically unrelated words should be further apart.\n",
    "\n",
    "The scipy library has a lot of vector comparison models methods, and for the purposes of our work here we'll be using this library as a supporting implementation of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The first comparison methodology we'll look at is Euclidean Distance. To find this distance, \n",
    "\n",
    "The equation to find the Euclidean Distance between vectors $u$ and $v$ of $n$ dimensions is below :\n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^{n} |u_{i}-v_{i}|^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In two dimensions, this corresponds to the length of the direct most line between the two points.\n",
    "\n",
    "Below is a method to define that function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to compute euclidean distance\n",
    "# we exploit a method already defined in scipy\n",
    "\n",
    "def euclidean(u, v):\n",
    "    return scipy.spatial.distance.euclidean(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create a small toy array (matrix) for use within our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Two dimensional vector embedding in two dimensional space to show \n",
    "# how we can measure the distance between two vectors in 2 space\n",
    "\n",
    "ABC = np.array([\n",
    "    [ 2.0,  4.0],  # 0\n",
    "    [ 1.50, 3.50], # 1\n",
    "    [10.0, 15.0],  # 2\n",
    "    [14.0, 10.0]]) # 3\n",
    "\n",
    "def plot_ABC(m):\n",
    "    plt.plot(m[:,0], m[:,1], marker='', linestyle='')\n",
    "    plt.xlim([0,np.max(m)*1.2])\n",
    "    plt.ylim([0,np.max(m)*1.2])\n",
    "    for i, x in enumerate(['0','1','2','3']):\n",
    "        plt.annotate(x, m[i,:])\n",
    "\n",
    "plot_ABC(ABC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Measure the distance between vectors A and B in matrix ABC\n",
    "euclidean(ABC[0], ABC[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Measure the distance between vectors B and C in matrix ABC\n",
    "euclidean(ABC[0], ABC[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Length\n",
    "\n",
    "Euclidean distance measures the difference between two vector lengths, as shown the in the equation above. We're also able to find the length of a single vector, for use in later examples, as well by using the following equation :\n",
    "\n",
    "$$\\|u\\| = \\sqrt{\\sum_{i=1}^{n} u_{i}^{2}}$$\n",
    "\n",
    "We use the numpy library sqrt and dot product methods within the library shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_length(u):\n",
    "    return np.sqrt(np.dot(u, u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Normalization\n",
    "\n",
    "When working with datasets that have large(r), variation in the size of the datapoints that are used can skew the actual distance measured between vectors. Below we define a normalization function that will normalize each vector according to its length.\n",
    "\n",
    "When normalizing all vectors according to their length, we can see in the plot below that this changes the representation of the data quite and bit and actually brings vector 0 and 1 closer together on the plot, showing their stronger similarity than say vectors 2 and 3, or 0 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_norm(u):\n",
    "    return  u / vector_length(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ABC(np.array([length_norm(row) for row in ABC]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Distance\n",
    "\n",
    "The cosine distance takes the overall vector length into account, meaning we don't have to run the vector_length() method over the vectors before calculation, and measure the angle between the two vectors. This is all captured within the Cosine Distance measurement function that is seen below :\n",
    "\n",
    "$$1 - \\left(\\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|\\cdot \\|v\\|}\\right)$$\n",
    "\n",
    "The result is the same as first normalizing the vectors according to their length, vector_length(), and then computing the Euclidean distance between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    # Use scipy's method:\n",
    "    return scipy.spatial.distance.cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine(ABC[1],ABC[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Neighbors\n",
    "\n",
    "This functional is an investigational aide and allows us to use the similarity functions defined above over the IMDB dataset that we've previously loaded.\n",
    "\n",
    "This function is performing a cosine similarity computation ranking the _rownames_ according to their distance from _word_, as measured by the distfunc in matrix mat.\n",
    "\n",
    "The function returns a sorted list of words that ascend from the closest, distance wise, depending on which distance method is passed in. The method defaults to using the cosine distance measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neighbors(word=None, mat=None, rownames=None, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this dataset' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], distfunc(w, mat[i])) for i in range(len(mat))]\n",
    "    return sorted(dists, key=itemgetter(1), reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neighbors method above might be a bit confusing, as its performing the cosine calculation over all vectors of co-occurences, per pair, within the dataset that's provided. And returning a sorted list of closest words to the input word.\n",
    "\n",
    "That said, here I provide a simple single word comparison that is then extrapolated across all words in the co-occurance matrix, within the neighbors function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1 = ww[0][ww[1].index('superb')]\n",
    "\n",
    "w2 = ww[0][ww[1].index('wonderfully')]\n",
    "\n",
    "print(\"The vector representation for w1 is : \" + str(w1))\n",
    "\n",
    "print(\"The vector representation for w2 is : \" + str(w2))\n",
    "\n",
    "print(\"The cosine distance between vectors w1 and w2 is : \" + str(cosine(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking this down even further, instead of allowing the cosine distance function to perform the normalization, and Euclidean distance calculations, we can do it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generic manual calculation of the Euclidean distance with the dot product of both vectors divided by\n",
    "# the length of the vectors, also known as the L2 norm of those vectors\n",
    "manual = 1.0 - (np.dot(w1, w2) / (vector_length(w1) * vector_length(w2)))\n",
    "\n",
    "manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show an example of a word and their respective neighbors according to both cosine similarity and basic euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighbors(word='superb', mat=ww[0], rownames=ww[1], distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighbors(word='superb', mat=ww[0], rownames=ww[1], distfunc=euclidean)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe representation\n",
    "\n",
    "Here we can see that the GloVe vectors perform _very_ well, but they are based on much more than a simple raw count of occurences in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighbors(word='superb', mat=glv[0], rownames=glv[1], distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Reweighting\n",
    "\n",
    "Taking a look at the similarity measurements listed above, we can see that they offer some insight into the embedded context contained within written language, but they're not quite as accurate as we would like them to be. This is where matrix reweighting comes into play. The goal of matrix reweighting is to amplify the important, trustworthy and unusual representations found within language, and de-emphasize the unimportant, and untrustworthy representations.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Here we see another form of normalization that can be applied to the data. With length_norm() seen above, we normalize using the vector_length(). There are other ways that we can normalize the datasets as well, such as each row by the sum of its values, which turns each row into a probability distribution over the columns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_norm(u):\n",
    "    return u / np.sum(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted, however, that these normalization measures are insensitive to the magnitude of the underlying counts, which can cause problems in larger datasets. An example is [1,10] is vastly different than [1000, 10000], and those differences will end up being obscured by these normalization procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information [1]\n",
    "\n",
    "This brings us to something called Pointwise Mutual Information (PMI), which can help address the issue of the ignorance of the magnitude of the counts within the previous normalization functions. This is the measure between a pair of discreet outcomes $x$ and $y$, as defined as :\n",
    "\n",
    "$$PMI(x,y) = log \\frac{P(x,y)}{P(x) \\cdot P(y)}$$\n",
    "\n",
    "$PMI(w,c)$ measures the association between a word $w$ and a context $c$ by calculating the log of the ratio between their joint probability (the frequency in which they occur together) and their marginal probabilities (the frequency with which they occur independently).\n",
    "\n",
    "This can be shown by considering the actual number of observations within a corpus :\n",
    "\n",
    "$$PMI(w,c) = log \\frac{\\#(w,c) \\cdot |D|}{\\#(w) \\cdot \\#(c)}$$\n",
    "\n",
    "The challenge with PMI arises when we compute $M^{PMI}$. This leaves us with rows containing many entries of word-context pairs $(w,c)$ that were never observed in the corpus, for which $PMI(w,c) = log 0 = -\\infty$. This leaves the matrix 'ill-defined'. We could do potentially smooth over the probabilities using something like a Dirichlet prior by adding a small \"fake\" count to the underlying counts matrix. But this leaves us with, still, a very dense matrix to deal with.\n",
    "\n",
    "An alternative approach would be to replace $M^{pmi}$ with $M_0^{PMI}$, in which $PMI(w,c) = 0$ in all cases $\\#(w,c) = 0$, which results in a sparse matrix.\n",
    "\n",
    "When approaching this problem, we understand that there are \"bad\" (uncorrelated) word-context pairs, which are represented with negative matrix entries, and replacing $M^{PMI}$ with $M_0^{PMI}$ will leave non-existent word context pairs with 0 entries in the matrix, which would rate them \"better\" than resulting negative entries from the real $M^{PMI}$.\n",
    "\n",
    "### Positive Pointwise Mutual Information [1]\n",
    "\n",
    "To get around the problem framed above, where uncorrelated word-context pairs will have negative matrix entries whereas zeroed out entries would exist for non-existent word-context pairs, we're able to a modification to the PMI algorithm called Positive Pointwise Mutual Information (PPMI). This will map all negative values within the matrix to 0.0, leveling the playing field between unseen word-context pairs and negative valued entries within the matrix. We can view this new positive PMI matrix as $M^{PPMI}$.\n",
    "\n",
    "$$ PPMI(w,c) = max(PMI(w,c),0) $$\n",
    "\n",
    "Below we capture both implementations, PMI and PPMI. The positive argument default to True, implementing PPMI, if this argument is set to False, it will perform regular PMI.\n",
    "\n",
    "It's interesting to note that the PMI matrix $M^{PMI}$ is what emerges as the optimal solution for SGNS, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PMI example\n",
    "def pmi(mat=None, rownames=None, positive=True):\n",
    "    # Create a joint probability table\n",
    "    p = mat / np.sum(mat, axis=None)\n",
    "    # Pre-compute column sums\n",
    "    colprobs = np.sum(p, axis=0)\n",
    "    # Vectorize this function so it can be applied rowwise\n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))\n",
    "    p = np.array([np_pmi_log(row / (np.sum(row)*colprobs)) for row in p])\n",
    "    return (p, rownames)\n",
    "\n",
    "def _pmi_log(x, positive=True):\n",
    "    \"\"\"Where positive = False, return log(x) if possible, else 0.\n",
    "    Where positive = True, log(x) is mapped to 0 where negative.\"\"\"\n",
    "    # set val var to 0.0\n",
    "    val = 0.0\n",
    "    # conditional \n",
    "    if x > 0.0:\n",
    "        val = np.log(x)\n",
    "    if positive:\n",
    "        val = max([val, 0.0])\n",
    "    return val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process the ww data using PPMI\n",
    "ww_ppmi = pmi(mat=ww[0], rownames=ww[1], positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighbors(word='superb', mat=ww_ppmi[0], rownames=ww_ppmi[1], distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again with Euclidean distance measure :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighbors(word='superb', mat=ww_ppmi[0], rownames=ww_ppmi[1], distfunc=euclidean)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram Negative Sampling (SGNS)\n",
    "\n",
    "### Skipgram Model\n",
    "\n",
    "The skipgram model assumes a corpus of words $w \\in V_{w}$ and their contexts $c \\in V_{c}$, where $V_w$ and $V_c$ are word and context vocabularies. The context vacabulary was assembled using an _L_-sized window $w_{i-L}...,w_{i-1}, w_{i+1}...,w_{i+L}$. \n",
    "\n",
    "The objective of the skipgram model is to maximize the average log probability, where $c$ is the size of the training context, seen as a function of the center word $w_{i}$.\n",
    "\n",
    "As an example, say we set the window size to a count of 3, using the example sentence below, \"The quick brown fox jumped over the lazy dog\", if we choose 'jumped' as our word of interest, we can add it to $V_{w}$. After adding 'jumped' the the word vector, we can use the window function to capture the surrounding words by simply counting forward and backward through the sentence and append each word to $V_{c}$.\n",
    "\n",
    "<img src=\"SGNSwindow.jpg\">\n",
    "\n",
    "We denote the collection of observed word context pairs, $(w,c)$, as $D$. We also use $\\#(w,c)$ to denote the number of times observe $(w,c)$ in $D$.\n",
    "\n",
    "### SGNS Objective\n",
    "\n",
    "The objective of Skipgram with Negative Sampling is to maximize the the probability that $(w,c)$ came from the data $D$. This can be modeled as a distribution such that $P(D=1|w,c)$ be the probability that $(w,c)$ came from the data and $P(D=0|w,c) = 1 - P(D=1|w,c)$ the probability that $(w,c)$ did not. \n",
    "\n",
    "The distribution is modeled as :\n",
    "\n",
    "$$P(D=1|w,c) = \\sigma(\\vec{w} \\cdot \\vec{c}) = \\frac{1}{1+e^{-\\vec{w} \\cdot \\vec{c}}}$$\n",
    "\n",
    "where $\\vec{w}$ and $\\vec{c}$ (each a d-dimensional vector) are the model parameters to be learned.\n",
    "\n",
    "The negative sampling tries to maximize $P(D=1|w,c)$ for observed $(w,c)$ pairs while maximizing $P(D=0|w,c)$ for stochastically sampled \"negative\" examples, under the assumpting that selecting a contect for a given word is likely to result in an unobserved $(w,c)$ pair.\n",
    "\n",
    "SGNS's objective for a single $(w,c)$ observation is then:\n",
    "\n",
    "$$ log \\sigma(\\vec{w} \\cdot \\vec{c}) + k \\cdot \\mathbb{E}_{c_{N} \\sim P_{D}} [log \\sigma(\\vec{-w} \\cdot \\vec{c}_N)] $$\n",
    "\n",
    "where $k$ is the number of \"negative\" samples and $c_{N}$ is the sampled context, drawn according to the empirical unigram distribution $P_{D}(c) = \\frac{\\#c}{|D|}$.\n",
    "\n",
    "This object is then trained in an online fashion using stochastic gradient updated over the observed pairs in the corpus $D$. The goal objective then sums over the observed $(w,c)$ pairs in the corpus :\n",
    "\n",
    "$$ \\ell = \\Sigma_{w \\in V_{w}} \\Sigma_{c \\in V_{c}} \\#(w,c)(log \\sigma(\\vec{w} \\cdot \\vec{c}) + k \\cdot \\mathbb{E}_{c_{N} \\sim P_{D}} [log \\sigma(\\vec{-w} \\cdot \\vec{c}_N)]$$\n",
    "\n",
    "Optimizing this objective groups words that have similar embeddings, while scattering unobserved pairs.\n",
    "\n",
    "Tying the two of these together, we can see that SGNS with $k = 1$ is attempting to implicity factorize the familiar matrix $M^{PMI}$.\n",
    "\n",
    "Past re-implementing word2vec and all of the hyperparameters that have been discussed here, we can defer to the word2vec implementation that exists within the library gensim.\n",
    "\n",
    "Within gensim are two implementations of word2vec, both hierachical softmax and negative sampling. \n",
    "\n",
    "In order for us to be able to show how the skip-gram model, and ultimately negative sampling combined with skip-gram, works we will need an unprocessed dataset. We can see below how to import that and ready it for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'descendants'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c43a1fba07d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mraw_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aclImdb/train/unsup/*.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-c43a1fba07d4>\u001b[0m in \u001b[0;36mimport_raw_data\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileinput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/bs4/element.py\u001b[0m in \u001b[0;36mfind_all\u001b[0;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         same is true of the tag name.\"\"\"\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescendants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'descendants'"
     ]
    }
   ],
   "source": [
    "# Method used to import raw data\n",
    "def import_raw_data(dir):\n",
    "    corpora = []\n",
    "    filenames = glob(dir)\n",
    "    for line in fileinput.input(filenames):\n",
    "        soup.findAll(line)\n",
    "        corpora.append(line)\n",
    "    return corpora\n",
    "\n",
    "raw_comments = import_raw_data('aclImdb/train/unsup/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = BeautifulSoup(raw_comments[0], \"html.parser\")\n",
    "\n",
    "print(raw_comments[0])\n",
    "\n",
    "#print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = MySentences('aclImdb/train/unsup/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Omar Levy, Yoav Goldberg. Neural Word embedding as Implicit Matrix Factorization. NIPS, 2015\n",
    "\n",
    "[2] Omar Levy, Yoav Golderg. word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method\n",
    "\n",
    "[3] Omar Levy, Yoav Goldber, Ido Dagan. Improving Distributional Similarity with Lessons Learned from Word Embeddings\n",
    "\n",
    "[4] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys; print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

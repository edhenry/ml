{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "\n",
    "# Make sure you've got Numpy and Scipy installed:\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial.distance\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# For visualization:\n",
    "import tsne # See http://lvdmaaten.github.io/tsne/#implementations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For clustering in the 'Word-sense ambiguities' section:\n",
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Matrices\n",
    "\n",
    "We can start by creating a dictionary $d$ that can be used to map word-pairs to counts. Everytime a pair of words $w$ and $w^\\prime$ occur, we can increment a counter associated with each pair $d[w,w^\\prime]$ by $1$. Using these count dictionary we can then create our vocabulary $V$, an ordered list of words types.\n",
    "\n",
    "We can then create a matrix, $M$, of dimensions $|V|$ x $|V|$. Each $M[i,j]$ is filled with the counts contained in $d[w_i,w_j]$.\n",
    "\n",
    "These co-occurence matrices have been provided with this example. We can import them with the _build_ function seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build method to import co-occurence matrices\n",
    "\n",
    "def build(src_filename, delimiter=',', header=True, quoting=csv.QUOTE_MINIMAL):    \n",
    "    reader = csv.reader(file(src_filename), delimiter=delimiter, quoting=quoting)\n",
    "    colnames = None\n",
    "    if header:\n",
    "        colnames = reader.next()\n",
    "        colnames = colnames[1: ]\n",
    "    mat = []    \n",
    "    rownames = []\n",
    "    for line in reader:        \n",
    "        rownames.append(line[0])            \n",
    "        mat.append(np.array(map(float, line[1: ])))\n",
    "    return (np.array(mat), rownames, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now read in the example co-occurency matrices for later. We will be using the IMDB moview review dataset for these examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import word <-> word co-occurence matrix\n",
    "ww = build('distributedwordreps-data/imdb-wordword.csv')\n",
    "\n",
    "#Import w <-> document co-occurence matrix\n",
    "wd = build('distributedwordreps-data/imdb-worddoc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word co-occurences : !, and, !\n",
      "Count : 343744.0\n"
     ]
    }
   ],
   "source": [
    "# Example of counts of first words in the document which happen to be two exclamation marks\n",
    "\n",
    "print(\"Word co-occurences : \" + ww[1][0] + \", and, \" + ww[2][0])\n",
    "\n",
    "print \"Count : \" + str(ww[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load GloVe vectors as well\n",
    "\n",
    "glv = build('distributedwordreps-data/glove.6B.50d.txt', delimiter=' ', header=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Comparisons\n",
    "\n",
    "For the most part we are interested in measuring the _distance_ between two vectors. The general idea of vector comparisons using distance is that words that are semantically similar should be closer together in the vector spaces we build, and symantically unrelated words should be further apart.\n",
    "\n",
    "The scipy library has a lot of vector comparison models methods, and for the purposes of our work here we'll be using this library as a supporting implementation of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The first comparison methodology we'll look at is Euclidean Distance. To find this distance, \n",
    "\n",
    "The equation to find the Euclidean Distance between vectors $u$ and $v$ of $n$ dimensions is below :\n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^{n} |u_{i}-v_{i}|^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In two dimensions, this corresponds to the length of the direct most line between the two points.\n",
    "\n",
    "Below is a method to define that function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to compute euclidean distance\n",
    "# we exploit a method already defined in scipy\n",
    "\n",
    "def euclidean(u, v):\n",
    "    return scipy.spatial.distance.euclidean(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create a small toy array (matrix) for use within our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJze7/AGjJTNevK1NEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1128d1bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Two dimensional vector embedding in two dimensional space to show \n",
    "# how we can measure the distance between two vectors in 2 space\n",
    "\n",
    "ABC = np.array([\n",
    "    [ 2.0,  4.0],  # 0\n",
    "    [ 1.50,  3.50],# 1\n",
    "    [10.0, 15.0],  # 2\n",
    "    [14.0, 10.0]]) # 3\n",
    "\n",
    "def plot_ABC(m):\n",
    "    plt.plot(m[:,0], m[:,1], marker='', linestyle='')\n",
    "    plt.xlim([0,np.max(m)*1.2])\n",
    "    plt.ylim([0,np.max(m)*1.2])\n",
    "    for i, x in enumerate(['0','1','2','3']):\n",
    "        plt.annotate(x, m[i,:])\n",
    "\n",
    "plot_ABC(ABC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70710678118654757"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measure the distance between vectors A and B in matrix ABC\n",
    "euclidean(ABC[0], ABC[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measure the distance between vectors B and C in matrix ABC\n",
    "euclidean(ABC[0], ABC[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Length\n",
    "\n",
    "Euclidean distance measures the difference between two vector lengths, as shown the in the equation above. We're also able to find the length of a single vector, for use in later examples, as well by using the following equation :\n",
    "\n",
    "$$\\|u\\| = \\sqrt{\\sum_{i=1}^{n} u_{i}^{2}}$$\n",
    "\n",
    "We use the numpy library sqrt and dot product methods within the library shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_length(u):\n",
    "    return np.sqrt(np.dot(u, u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Normalization\n",
    "\n",
    "When working with datasets that have large(r), variation in the size of the datapoints that are used can skew the actual distance measured between vectors. Below we define a normalization function that will normalize each vector according to its length.\n",
    "\n",
    "When normalizing all vectors according to their length, we can see in the plot below that this changes the representation of the data quite and bit and actually brings vector A and B closer together on the plot, showing their stronger similarity than say vectors B and C, or A and C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_norm(u):\n",
    "    return  u / vector_length(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD7CAYAAAClvBX1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naO/ckiRJkiRJkiRJkiRJkiRJkiRJkvT/L0ZWYPSKPmgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d198790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_ABC(np.array([length_norm(row) for row in ABC]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Distance\n",
    "\n",
    "The cosine distance takes the overall vector length into account, meaning we don't have to run the vector_length() method over the vectors before calculation, and measure the angle between the two vectors. This is all captured within the Cosine Distance measurement function that is seen below :\n",
    "\n",
    "$$1 - \\left(\\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|\\cdot \\|v\\|}\\right)$$\n",
    "\n",
    "The result is the same as first normalizing the vectors according to their length, vector_length(), and then computing the Euclidean distance between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    # Use scipy's method:\n",
    "    return scipy.spatial.distance.cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14521248477827109"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ABC[1],ABC[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Neighbors\n",
    "\n",
    "This functional is an investigational aide and allows us to use the similarity functions defined above over the IMDB dataset that we've previously loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neighbors(word=None, mat=None, rownames=None, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], distfunc(w, mat[i])) for i in xrange(len(mat))]\n",
    "    return sorted(dists, key=itemgetter(1), reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show an example of a word and their respective neighbors according to both cosine similarity and basic euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', 0.0),\n",
       " ('excellent', 0.0026965023912962627),\n",
       " ('outstanding', 0.0027344413235226295),\n",
       " ('beautifully', 0.0027345163104325332),\n",
       " ('brilliant', 0.0027888643627086429),\n",
       " ('performances', 0.0028333319740448948),\n",
       " ('perfectly', 0.0028436893209292657),\n",
       " ('memorable', 0.0028935533453889883),\n",
       " ('cinematography', 0.0029206920379420964),\n",
       " ('wonderfully', 0.0029473105937050104)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(word='superb', mat=ww[0], rownames=ww[1], distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', 0.0),\n",
       " ('familiar', 1448.8919904533948),\n",
       " ('violent', 1630.3723501090174),\n",
       " ('follows', 1647.0276257549538),\n",
       " ('convincing', 1701.2260284865147),\n",
       " ('pace', 1748.1195611284716),\n",
       " ('recent', 1769.8525362300668),\n",
       " ('amount', 1783.1822677449436),\n",
       " ('impressive', 1789.4510331383756),\n",
       " ('masterpiece', 1849.5091240650856)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(word='superb', mat=ww[0], rownames=ww[1], distfunc=euclidean)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe representation\n",
    "\n",
    "Here we can see that the GloVe vectors perform _very_ well, but they are based on much more than a simple raw count of occurences in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', 2.2204460492503131e-16),\n",
       " ('brilliant', 0.15809110259014747),\n",
       " ('impressive', 0.19352861376442654),\n",
       " ('masterful', 0.22871323564771928),\n",
       " ('excellent', 0.22928471014596696),\n",
       " ('volley', 0.24414121851443737),\n",
       " ('deft', 0.24755074890723616),\n",
       " ('dazzling', 0.26191184828838354),\n",
       " ('score', 0.26231230884814061),\n",
       " ('scoring', 0.26641165920991983)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(word='superb', mat=glv[0], rownames=glv[1], distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Reweighting\n",
    "\n",
    "Taking a look at the similarity measurements listed above, we can see that they offer some insight into the embedded context contained within written language, but they're not quite as accurate as we would like them to be. This is where matrix reweighting comes into play. The goal of matrix reweighting is to amplify the important, trustworthy and unusual representations found within language, and de-emphasize the unimportant, and untrustworthy representations.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Here we see another form of normalization that can be applied to the data. With length_norm() seen above, we normalize using the vector_length(). There are other ways that we can normalize the datasets as well, such as each row by the sum of its values, which turns each row into a probability distribution over the columns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_norm(u):\n",
    "    return u / np.sum(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These normalization measures are insensitive to the magnitude of the underlying counts, which can cause problems in larger datasets. An example is [1,10] is vastly different than [1000, 10000], and those differences will end up being obscured by these normalization procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Mutual Information\n",
    "\n",
    "This brings us to something called Pointwise Mutual Information (PMI), which can help address the issue of the ignorance of the magnitude of the counts within the previous normalization functions. This is the measure between a pair of discreet outcomes $x$ and $y$, as defined as :\n",
    "\n",
    "$$PMI(x,y) = log \\frac{P(x,y)}{P(x) \\cdot P(y)}$$\n",
    "\n",
    "$PMI(w,c)$ measures the association between a word $w$ and a context $c$ by calculating the log of the ratio between their joint probability (the frequency in which they occur together) and their marginal probabilities (the frequency with which they occur independently).\n",
    "\n",
    "This can be shown by considering the actual number of observations within a corpus :\n",
    "\n",
    "$$PMI(w,c) = log \\frac{\\#(w,c) \\cdot |D|}{\\#(w) \\cdot \\#(c)}$$\n",
    "\n",
    "The challenge with PMI arises when we compute $M^{PMI}$. This leaves us with rows containing many entries of word-context pairs $(w,c)$ that were never observed in the corpus, for which $PMI(w,c) = log 0 = -\\infty$. This leaves the matrix 'ill-defined'. We could do potentially smooth over the probabilities using something like a Dirichlet prior by adding a small \"fake\" count to the underlying counts matrix. But this leaves us with, still, a very dense matrix to deal with.\n",
    "\n",
    "An alternative approach would be to replace $M^{pmi}$ with $M_0^{PMI}$, in which $PMI(w,c) = 0$ in all cases $\\#(w,c) = 0$, which results in a sparse matrix.\n",
    "\n",
    "When approaching this problem, we understand that there are \"bad\" (uncorrelated) word-context pairs, which are represented with negative matrix entries, and replacing $M^{PMI}$ with $M_0^{PMI}$ will leave non-existent word context pairs with 0 entries in the matrix, which would rate them \"better\" than resulting negative entries from the real $M^{PMI}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PMI example\n",
    "def pmi(mat=None, rownames=None):\n",
    "    # Create a joint probability table\n",
    "    p = mat / np.sum(mat, axis=None)\n",
    "    # Pre-compute column sums\n",
    "    colprobs = np.sum(p, axis=0)\n",
    "    # Vectorize this function so it can be applied rowwise\n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))\n",
    "    p = np.array([np_pmi_log(row / (np.sum(row)*colprobs)) for row in p])\n",
    "    return (p, rownames)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

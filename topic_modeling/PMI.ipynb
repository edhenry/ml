{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "# Make sure you've got Numpy and Scipy installed:\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial.distance\n",
    "from six import iteritems, itervalues, string_types\n",
    "\n",
    "# Visualization eff-yeah\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# word2vec\n",
    "import gensim\n",
    "import fileinput\n",
    "from glob import glob\n",
    "\n",
    "# For cleaning HTML mined data\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "# Regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Matrices\n",
    "\n",
    "We can start by creating a dictionary $d$ that can be used to map word-pairs to counts. Everytime a pair of words $w$ and $w^\\prime$ occur, we can increment a counter associated with each pair $d[w,w^\\prime]$ by $1$. Using these count dictionary we can then create our vocabulary $V$, an ordered list of words types.\n",
    "\n",
    "We can then create a matrix, $M$, of dimensions $|V|$ x $|V|$. Each $M[i,j]$ is filled with the counts contained in $d[w_i,w_j]$.\n",
    "\n",
    "These co-occurence matrices have been provided with this example. We can import them with the _build_ function seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build method to import co-occurence matrices\n",
    "\n",
    "def build(src_filename, delimiter=',', header=True, quoting=csv.QUOTE_MINIMAL):    \n",
    "    reader = csv.reader(open(src_filename), delimiter=delimiter, quoting=quoting)\n",
    "    colnames = None\n",
    "    if header:\n",
    "        colnames = reader.__next__()\n",
    "        colnames = colnames[1: ]\n",
    "    mat = []    \n",
    "    rownames = []\n",
    "    for line in reader:\n",
    "        rownames.append(line[0])            \n",
    "        mat.append(np.array(list(map(float, line[1: ]))))    \n",
    "    return (np.array(mat), rownames, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now read in the example co-occurency matrices for later. We will be using the IMDB moview review dataset for these examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import word <-> word co-occurence matrix\n",
    "ww = build('distributedwordreps-data/imdb-wordword.csv')\n",
    "\n",
    "#Import w <-> document co-occurence matrix\n",
    "wd = build('distributedwordreps-data/imdb-worddoc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343744.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word co-occurences : !, and, !\n",
      "Count : 343744.0\n"
     ]
    }
   ],
   "source": [
    "# Example of counts of first words in the document which happen to be two exclamation marks\n",
    "\n",
    "print(\"Word co-occurences : \" + ww[1][0] + \", and, \" + ww[2][0])\n",
    "\n",
    "print(\"Count : \" + str(ww[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load GloVe vectors as well\n",
    "\n",
    "glv = build('distributedwordreps-data/glove.6B.50d.txt', delimiter=' ', header=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Comparisons\n",
    "\n",
    "For the most part we are interested in measuring the _distance_ between two vectors. The general idea of vector comparisons using distance is that words that are semantically similar should be closer together in the vector spaces we build, and symantically unrelated words should be further apart.\n",
    "\n",
    "The scipy library has a lot of vector comparison models methods, and for the purposes of our work here we'll be using this library as a supporting implementation of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The first comparison methodology we'll look at is Euclidean Distance. To find this distance, \n",
    "\n",
    "The equation to find the Euclidean Distance between vectors $u$ and $v$ of $n$ dimensions is below :\n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^{n} |u_{i}-v_{i}|^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In two dimensions, this corresponds to the length of the direct most line between the two points.\n",
    "\n",
    "Below is a method to define that function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to compute euclidean distance\n",
    "# we exploit a method already defined in scipy\n",
    "\n",
    "def euclidean(u, v):\n",
    "    return scipy.spatial.distance.euclidean(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create a small toy array (matrix) for use within our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngWASAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f96b400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Two dimensional vector embedding in two dimensional space to show \n",
    "# how we can measure the distance between two vectors in 2 space\n",
    "\n",
    "ABC = np.array([\n",
    "    [ 2.0,  4.0],  # 0\n",
    "    [ 1.50, 3.50], # 1\n",
    "    [10.0, 15.0],  # 2\n",
    "    [14.0, 10.0]]) # 3\n",
    "\n",
    "def plot_ABC(m):\n",
    "    plt.plot(m[:,0], m[:,1], marker='', linestyle='')\n",
    "    plt.xlim([0,np.max(m)*1.2])\n",
    "    plt.ylim([0,np.max(m)*1.2])\n",
    "    for i, x in enumerate(['0','1','2','3']):\n",
    "        plt.annotate(x, m[i,:])\n",
    "\n",
    "plot_ABC(ABC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865476"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measure the distance between vectors A and B in matrix ABC\n",
    "euclidean(ABC[0], ABC[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865476"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measure the distance between vectors B and C in matrix ABC\n",
    "euclidean(ABC[0], ABC[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Length\n",
    "\n",
    "Euclidean distance measures the difference between two vector lengths, as shown the in the equation above. We're also able to find the length of a single vector, for use in later examples, as well by using the following equation :\n",
    "\n",
    "$$\\|u\\| = \\sqrt{\\sum_{i=1}^{n} u_{i}^{2}}$$\n",
    "\n",
    "We use the numpy library sqrt and dot product methods within the library shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_length(u):\n",
    "    return np.sqrt(np.dot(u, u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Normalization\n",
    "\n",
    "When working with datasets that have large(r), variation in the size of the datapoints that are used can skew the actual distance measured between vectors. Below we define a normalization function that will normalize each vector according to its length.\n",
    "\n",
    "When normalizing all vectors according to their length, we can see in the plot below that this changes the representation of the data quite and bit and actually brings vector 0 and 1 closer together on the plot, showing their stronger similarity than say vectors 2 and 3, or 0 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_norm(u):\n",
    "    return  u / vector_length(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD7CAYAAACc26SuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11dfcabe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_ABC(np.array([length_norm(row) for row in ABC]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Distance\n",
    "\n",
    "The cosine distance takes the overall vector length into account, meaning we don't have to run the vector_length() method over the vectors before calculation, and measure the angle between the two vectors. This is all captured within the Cosine Distance measurement function that is seen below :\n",
    "\n",
    "$$1 - \\left(\\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|\\cdot \\|v\\|}\\right)$$\n",
    "\n",
    "The result is the same as first normalizing the vectors according to their length, vector_length(), and then computing the Euclidean distance between the two.\n",
    "\n",
    "One thing to note with respect to Cosine Distance, is it should be viewed as the Cosine Similarity complement in positive space, such that :\n",
    "\n",
    "$$D_{c}(A,B) = 1 - S_{c}(A,B)$$\n",
    "\n",
    "Meaning that the cosine distance is a proper distance metric, which is not the case for Cosine Similarity as it does not have the triangle inequality property. This, however, is not necessarily an important property for the operations we'll be performing.[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    # Use scipy's method:\n",
    "    return scipy.spatial.distance.cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14521248477827109"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ABC[1],ABC[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Neighbors\n",
    "\n",
    "This functional is an investigational aide and allows us to use the similarity functions defined above over the IMDB dataset that we've previously loaded.\n",
    "\n",
    "This function is performing a cosine similarity computation ranking the _rownames_ according to their distance from _word_, as measured by the distfunc in matrix mat.\n",
    "\n",
    "The function returns a sorted list of words that ascend from the closest, distance wise, depending on which distance method is passed in. The method defaults to using the cosine distance measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neighbors(word=None, mat=None, rownames=None, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this dataset' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], distfunc(w, mat[i])) for i in range(len(mat))]\n",
    "    return sorted(dists, key=itemgetter(1), reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neighbors method above might be a bit confusing, as its performing the cosine calculation over all vectors of co-occurences, per pair, within the dataset that's provided. And returning a sorted list of closest words to the input word.\n",
    "\n",
    "That said, here I provide a simple single word comparison that is then extrapolated across all words in the co-occurance matrix, within the neighbors function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector representation for w1 is : [ 381.   11.    4. ...,    7.    0.    1.]\n",
      "The vector representation for w2 is : [ 207.    1.    5. ...,    2.   13.   10.]\n",
      "The cosine distance between vectors w1 and w2 is : 0.00294731059371\n"
     ]
    }
   ],
   "source": [
    "w1 = ww[0][ww[1].index('superb')]\n",
    "\n",
    "w2 = ww[0][ww[1].index('wonderfully')]\n",
    "\n",
    "print(\"The vector representation for w1 is : \" + str(w1))\n",
    "\n",
    "print(\"The vector representation for w2 is : \" + str(w2))\n",
    "\n",
    "print(\"The cosine distance between vectors w1 and w2 is : \" + str(cosine(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking this down even further, instead of allowing the cosine distance function to perform the normalization, and Euclidean distance calculations, we can do it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0029473105937050104"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generic manual calculation of the Euclidean distance with the dot product of both vectors divided by\n",
    "# the length of the vectors, also known as the L2 norm of those vectors\n",
    "manual = 1.0 - (np.dot(w1, w2) / (vector_length(w1) * vector_length(w2)))\n",
    "\n",
    "manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show an example of a word and their respective neighbors according to both cosine similarity and basic euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', 0.0),\n",
       " ('excellent', 0.0026965023912962627),\n",
       " ('outstanding', 0.0027344413235226295),\n",
       " ('beautifully', 0.0027345163104325332),\n",
       " ('brilliant', 0.0027888643627086429),\n",
       " ('performances', 0.0028333319740448948),\n",
       " ('perfectly', 0.0028436893209292657),\n",
       " ('memorable', 0.0028935533453889883),\n",
       " ('cinematography', 0.0029206920379420964),\n",
       " ('wonderfully', 0.0029473105937050104)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(word='superb', mat=ww[0], rownames=ww[1], distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', 0.0),\n",
       " ('familiar', 1448.8919904533948),\n",
       " ('violent', 1630.3723501090174),\n",
       " ('follows', 1647.0276257549538),\n",
       " ('convincing', 1701.2260284865147),\n",
       " ('pace', 1748.1195611284716),\n",
       " ('recent', 1769.8525362300668),\n",
       " ('amount', 1783.1822677449436),\n",
       " ('impressive', 1789.4510331383756),\n",
       " ('masterpiece', 1849.5091240650856)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(word='superb', mat=ww[0], rownames=ww[1], distfunc=euclidean)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe representation\n",
    "\n",
    "Here we can see that the GloVe vectors perform _very_ well, but they are based on much more than a simple raw count of occurences in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', -4.4408920985006262e-16),\n",
       " ('brilliant', 0.15809110259014736),\n",
       " ('impressive', 0.19352861376442632),\n",
       " ('masterful', 0.22871323564771895),\n",
       " ('excellent', 0.22928471014596674),\n",
       " ('volley', 0.24414121851443749),\n",
       " ('deft', 0.24755074890723616),\n",
       " ('dazzling', 0.26191184828838354),\n",
       " ('score', 0.26231230884814061),\n",
       " ('scoring', 0.26641165920991983)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(word='superb', mat=glv[0], rownames=glv[1], distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Reweighting\n",
    "\n",
    "Taking a look at the similarity measurements listed above, we can see that they offer some insight into the embedded context contained within written language, but they're not quite as accurate as we would like them to be. This is where matrix reweighting comes into play. The goal of matrix reweighting is to amplify the important, trustworthy and unusual representations found within language, and de-emphasize the unimportant, and untrustworthy representations.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Here we see another form of normalization that can be applied to the data. With length_norm() seen above, we normalize using the vector_length(). There are other ways that we can normalize the datasets as well, such as each row by the sum of its values, which turns each row into a probability distribution over the columns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_norm(u):\n",
    "    return u / np.sum(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted, however, that these normalization measures are insensitive to the magnitude of the underlying counts, which can cause problems in larger datasets. An example is [1,10] is vastly different than [1000, 10000], and those differences will end up being obscured by these normalization procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information [1]\n",
    "\n",
    "This brings us to something called Pointwise Mutual Information (PMI), which can help address the issue of the ignorance of the magnitude of the counts within the previous normalization functions. This is the measure between a pair of discreet outcomes $x$ and $y$, as defined as :\n",
    "\n",
    "$$PMI(x,y) = \\log \\frac{P(x,y)}{P(x) \\cdot P(y)}$$\n",
    "\n",
    "$PMI(w,c)$ measures the association between a word $w$ and a context $c$ by calculating the log of the ratio between their joint probability (the frequency in which they occur together) and their marginal probabilities (the frequency with which they occur independently).\n",
    "\n",
    "This can be shown by considering the actual number of observations within a corpus :\n",
    "\n",
    "$$PMI(w,c) = \\log \\frac{\\#(w,c) \\cdot |D|}{\\#(w) \\cdot \\#(c)}$$\n",
    "\n",
    "The challenge with PMI arises when we compute $M^{PMI}$. This leaves us with rows containing many entries of word-context pairs $(w,c)$ that were never observed in the corpus, for which $PMI(w,c) = \\log 0 = -\\infty$. This leaves the matrix 'ill-defined'. We could do potentially smooth over the probabilities using something like a Dirichlet prior by adding a small \"fake\" count to the underlying counts matrix. But this leaves us with, still, a very dense matrix to deal with.\n",
    "\n",
    "An alternative approach would be to replace $M^{pmi}$ with $M_0^{PMI}$, in which $PMI(w,c) = 0$ in all cases $\\#(w,c) = 0$, which results in a sparse matrix.\n",
    "\n",
    "When approaching this problem, we understand that there are \"bad\" (uncorrelated) word-context pairs, which are represented with negative matrix entries, and replacing $M^{PMI}$ with $M_0^{PMI}$ will leave non-existent word context pairs with 0 entries in the matrix, which would rate them \"better\" than resulting negative entries from the real $M^{PMI}$.\n",
    "\n",
    "### Positive Pointwise Mutual Information [1]\n",
    "\n",
    "To get around the problem framed above, where uncorrelated word-context pairs will have negative matrix entries whereas zeroed out entries would exist for non-existent word-context pairs, we're able to a modification to the PMI algorithm called Positive Pointwise Mutual Information (PPMI). This will map all negative values within the matrix to 0.0, leveling the playing field between unseen word-context pairs and negative valued entries within the matrix. We can view this new positive PMI matrix as $M^{PPMI}$.\n",
    "\n",
    "$$ PPMI(w,c) = \\max(PMI(w,c),0) $$\n",
    "\n",
    "Below we capture both implementations, PMI and PPMI. The positive argument default to True, implementing PPMI, if this argument is set to False, it will perform regular PMI.\n",
    "\n",
    "It's interesting to note that the PMI matrix $M^{PMI}$ is what emerges as the optimal solution for SGNS, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PMI example\n",
    "def pmi(mat=None, rownames=None, positive=True):\n",
    "    # Create a joint probability table\n",
    "    p = mat / np.sum(mat, axis=None)\n",
    "    # Pre-compute column sums\n",
    "    colprobs = np.sum(p, axis=0)\n",
    "    # Vectorize this function so it can be applied rowwise\n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))\n",
    "    p = np.array([np_pmi_log(row / (np.sum(row)*colprobs)) for row in p])\n",
    "    return (p, rownames)\n",
    "\n",
    "def _pmi_log(x, positive=True):\n",
    "    \"\"\"Where positive = False, return log(x) if possible, else 0.\n",
    "    Where positive = True, log(x) is mapped to 0 where negative.\"\"\"\n",
    "    # set val var to 0.0\n",
    "    val = 0.0\n",
    "    # conditional \n",
    "    if x > 0.0:\n",
    "        val = np.log(x)\n",
    "    if positive:\n",
    "        val = max([val, 0.0])\n",
    "    return val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process the ww data using PPMI\n",
    "ww_ppmi = pmi(mat=ww[0], rownames=ww[1], positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', -2.2204460492503131e-16),\n",
       " ('excellent', 0.41348274842943578),\n",
       " ('performances', 0.4439162856870249),\n",
       " ('brilliant', 0.45785117509986151),\n",
       " ('performance', 0.46856555779383202),\n",
       " ('outstanding', 0.48315645713600386),\n",
       " ('beautifully', 0.48570217706133567),\n",
       " ('stunning', 0.48653666411656982),\n",
       " ('as', 0.4896847339821726),\n",
       " ('brilliantly', 0.48998750813252601)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(word='superb', mat=ww_ppmi[0], rownames=ww_ppmi[1], distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again with Euclidean distance measure :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', 0.0),\n",
       " ('excellent', 12.876470234708654),\n",
       " ('great', 13.475381696026227),\n",
       " ('performances', 13.604470031275069),\n",
       " ('as', 13.805235205825177),\n",
       " ('performance', 13.826597314820859),\n",
       " ('role', 13.84978951697956),\n",
       " ('cast', 13.925190826368734),\n",
       " ('brilliant', 13.932513494981505),\n",
       " ('story', 13.940602382288677)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(word='superb', mat=ww_ppmi[0], rownames=ww_ppmi[1], distfunc=euclidean)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram Negative Sampling (SGNS)\n",
    "\n",
    "### Skipgram Model\n",
    "\n",
    "The skipgram model assumes a corpus of words $w \\in V_{w}$ and their contexts $c \\in V_{c}$, where $V_w$ and $V_c$ are word and context vocabularies. The context vacabulary was assembled using an _L_-sized window $w_{i-L}...,w_{i-1}, w_{i+1}...,w_{i+L}$. \n",
    "\n",
    "The objective of the skipgram model is to maximize the average log probability, where $c$ is the size of the training context, seen as a function of the center word $w_{i}$.\n",
    "\n",
    "As an example, say we set the window size to a count of 3, using the example sentence below, \"The quick brown fox jumped over the lazy dog\", if we choose 'jumped' as our word of interest, we can add it to $V_{w}$. After adding 'jumped' the the word vector, we can use the window function to capture the surrounding words by simply counting forward and backward through the sentence and append each word to $V_{c}$.\n",
    "\n",
    "<img src=\"SGNSwindow.jpg\">\n",
    "\n",
    "We denote the collection of observed word context pairs, $(w,c)$, as $D$. We also use $\\#(w,c)$ to denote the number of times observe $(w,c)$ in $D$.\n",
    "\n",
    "### SGNS Objective\n",
    "\n",
    "The objective of Skipgram with Negative Sampling is to maximize the the probability that $(w,c)$ came from the data $D$. This can be modeled as a distribution such that $P(D=1|w,c)$ be the probability that $(w,c)$ came from the data and $P(D=0|w,c) = 1 - P(D=1|w,c)$ the probability that $(w,c)$ did not. \n",
    "\n",
    "The distribution is modeled as :\n",
    "\n",
    "$$P(D=1|w,c) = \\sigma(\\vec{w} \\cdot \\vec{c}) = \\frac{1}{1+e^{-\\vec{w} \\cdot \\vec{c}}}$$\n",
    "\n",
    "where $\\vec{w}$ and $\\vec{c}$ (each a d-dimensional vector) are the model parameters to be learned.\n",
    "\n",
    "The negative sampling tries to maximize $P(D=1|w,c)$ for observed $(w,c)$ pairs while maximizing $P(D=0|w,c)$ for stochastically sampled \"negative\" examples, under the assumption that selecting a context for a given word is likely to result in an unobserved $(w,c)$ pair.\n",
    "\n",
    "SGNS's objective for a single $(w,c)$ observation is then:\n",
    "\n",
    "$$ \\log \\sigma(\\vec{w} \\cdot \\vec{c}) + k \\cdot \\mathbb{E}_{c_{N} \\sim P_{D}} [\\log \\sigma(\\vec{-w} \\cdot \\vec{c}_N)] $$\n",
    "\n",
    "where $k$ is the number of \"negative\" samples and $c_{N}$ is the sampled context, drawn according to the empirical unigram distribution $P_{D}(c) = \\frac{\\#c}{|D|}$.\n",
    "\n",
    "This object is then trained in an online fashion using stochastic gradient updated over the observed pairs in the corpus $D$. The goal objective then sums over the observed $(w,c)$ pairs in the corpus :\n",
    "\n",
    "$$ \\ell = \\Sigma_{w \\in V_{w}} \\Sigma_{c \\in V_{c}} \\#(w,c)(\\log \\sigma(\\vec{w} \\cdot \\vec{c}) + k \\cdot \\mathbb{E}_{c_{N} \\sim P_{D}} [\\log \\sigma(\\vec{-w} \\cdot \\vec{c}_N)]$$\n",
    "\n",
    "Optimizing this objective groups words that have similar embeddings, while scattering unobserved pairs.\n",
    "\n",
    "Tying the two of these together, we can see that SGNS with $k = 1$ is attempting to implicity factorize the familiar matrix $M^{PMI}$.\n",
    "\n",
    "Past re-implementing word2vec and all of the hyperparameters that have been discussed here, we can defer to the word2vec implementation that exists within the library gensim.\n",
    "\n",
    "Within gensim are two implementations of word2vec, both hierachical softmax and negative sampling. \n",
    "\n",
    "In order for us to be able to show how the skip-gram model, and ultimately negative sampling combined with skip-gram, works we will need an unprocessed dataset. We can see below how to import that and ready it for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method used to import raw data\n",
    "def import_raw_data(directory):\n",
    "    corpora = []\n",
    "    filenames = glob(directory)\n",
    "    # Read each file in as a separate line\n",
    "    for line in fileinput.input(filenames):\n",
    "        corpora.append(line)\n",
    "    return corpora\n",
    "\n",
    "# Clean review of HTML tags, punctuation, etc.\n",
    "\n",
    "def clean_review(reviews):\n",
    "    corpora = []\n",
    "    for i in reviews:\n",
    "        # Clean HTML\n",
    "        review_text = soup(i, \"html.parser\")\n",
    "        # Clean punctuation\n",
    "        letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text.get_text())\n",
    "        # Split into individual words and convert to lower_case\n",
    "        words = letters_only.lower().split()\n",
    "        # Join words back together removing extreanous whitespacing\n",
    "        corpora.append(( \" \".join(words)))\n",
    "    return corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_comments = import_raw_data('aclImdb/train/unsup/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i admit the great majority of films released before say 1933 are just not for me of the dozen or so major silents i have viewed one i loved the crowd and two were very good the last command and city lights that latter chaplin circa 1931 so i was apprehensive about this one and humor is often difficult to appreciate uh enjoy decades later i did like the lead actors but thought little of the film one intriguing sequence early on the guys are supposed to get de loused and for about three minutes fully dressed do some schtick in the background perhaps three dozen men pass by all naked white and black wwi and for most their butts part or full backside are shown was this an early variation of beefcake courtesy of howard hughes\n"
     ]
    }
   ],
   "source": [
    "clean_example = clean_review(raw_comments)\n",
    "\n",
    "print(clean_example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split sentences into words for tokenization within word2vec\n",
    "corpora = []\n",
    "for s in clean_example:\n",
    "    words = s.split()\n",
    "    corpora.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a model according to the data that we have from the IMDB dataset, using the word2vec implementation within gensim. I will also expose some of the underbelly of the gensim implementation of word2vec to help illustrate the data manipulation going on 'under the hood' of the implementation. I am doing this as gensim has a rather mature implementation of word2vec, adapted from the original code from Google (http://word2vec.googlecode.com/svn/trunk/word2vec.c), which is far more mature, stable, and usable than what I could produce in a reasonable amount of time.\n",
    "\n",
    "Below we see we're defining a model object, in which we call the Word2vec class from gensim, passing in a few arguments :\n",
    "\n",
    "* corpora -- this is the dataset we've defined\n",
    "* min_count -- this is the minimum count of any given word within the corpora that we're passing in, if the count is below this, the word will be discarded from the vocabulary\n",
    "* window -- This is the context window that is used to capture context surrounding a given word $w_{i}$ for $V_{c}$ as represented above\n",
    "* workers -- this is the number of worker threads that will be used to train (can mean faster training times with more cores)\n",
    "\n",
    "min_count and window are both hyperparameters for this model and can change the accuracy of our results, but we'll get into that later. What I want to provide here is a little intuition as to what is going on, mathematically.\n",
    "\n",
    "One thing to keep in mind with the gensim implementation of word2vec, when calling similarity functions like this, is there is an init_sims() method that is invoked at the beginning of the similarity methods. If the 'replace' argument is set for this method, it will perform an optimization for memory consumption within the model. It will return the L2-normalized vectors of each word, instead of maintaining the entire vector representation in memory. \n",
    "\n",
    "Again, represented as :\n",
    "\n",
    "$$\\|v\\| = \\sqrt{\\sum_{i=1}^{n} v_{i}^{2}}$$\n",
    "\n",
    "This will effectively make the model \"read-only\", and can't continue 'training' it. Something to keep in mind if you'd like to save some resources on your machine if you're working through larger datasets using this write-up as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(corpora, min_count=5, window=15, workers=4, negative=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down step by step for a single comment, much like the one we printed above to validate out text parsing and formatting functions. For simplicity and robustness of the example, we'll start with a word that is further along in the sentence than the first word. This will help illustrate the windowing in a little more detail as well.\n",
    "\n",
    "First we'll need to create a few table objects to house some of the required data that we'll be using in our operations, such as total_word count, the vocab\n",
    "\n",
    "Keep in mind this example will be _very_ limited in its functionality as word2vec is designed to run over large corpora - that's where the algorithms power comes in, is with more data. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Carve off 20 words from the first sentence contained \n",
    "# within the corpora object above\n",
    "play_corpora = corpora[0]\n",
    "\n",
    "# Create raw vocabulary from given corpus (will be small\n",
    "# according to limited vocabulary we've carved off our\n",
    "# full corpus)\n",
    "\n",
    "total_words = 0\n",
    "\n",
    "vocab = defaultdict(int)\n",
    "\n",
    "for word in play_corpora:\n",
    "        vocab[word] += 1\n",
    "        \n",
    "total_words += sum(itervalues(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtype = np.dtype\n",
    "\n",
    "vocab_size = len(play_corpora)\n",
    "\n",
    "# Create cumulative table of word counts\n",
    "\n",
    "cum_table = np.zeros(vocab_size, dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually compute the cosine similarity between two word vectors that have been created using the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', -2.2204460492503131e-16),\n",
       " ('excellent', 0.41348274842943578),\n",
       " ('performances', 0.4439162856870249),\n",
       " ('brilliant', 0.45785117509986151),\n",
       " ('performance', 0.46856555779383202),\n",
       " ('outstanding', 0.48315645713600386),\n",
       " ('beautifully', 0.48570217706133567),\n",
       " ('stunning', 0.48653666411656982),\n",
       " ('as', 0.4896847339821726),\n",
       " ('brilliantly', 0.48998750813252601)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(word='superb', mat=ww_ppmi[0], rownames=ww_ppmi[1], distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Omar Levy, Yoav Goldberg. Neural Word embedding as Implicit Matrix Factorization. NIPS, 2015\n",
    "\n",
    "[2] Omar Levy, Yoav Golderg. word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method\n",
    "\n",
    "[3] Omar Levy, Yoav Goldber, Ido Dagan. Improving Distributional Similarity with Lessons Learned from Word Embeddings\n",
    "\n",
    "[4] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality.\n",
    "\n",
    "[5] Cosine Similarity https://en.wikipedia.org/wiki/Cosine_similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

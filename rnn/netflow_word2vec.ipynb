{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyhash\n",
    "import gensim\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "import concurrent.futures\n",
    "from pprint import pprint\n",
    "import random\n",
    "import mpld3\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable mpld3 for notebook\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "# Instantiate hasher object\n",
    "hasher = pyhash.city_64()\n",
    "\n",
    "# Method to strip white test\n",
    "def strip(text):\n",
    "    return text.strip()\n",
    "\n",
    "# Method to set dataframe entries to integers\n",
    "def make_int(text):\n",
    "    return int(text.strip(''))    \n",
    "\n",
    "# Method to match IP against flow srcIP\n",
    "def sort_ip_flow(ip):\n",
    "    # List to house flows when matches\n",
    "    flows_list = []\n",
    "    # Iterate over tcp_flows list\n",
    "    for flow in tcp_flows:   \n",
    "        # Comparison logic - flow[1][3] corresponds to SrcIP in flow tuple\n",
    "        if ip == flow[1][3]:        \n",
    "            # Append match to flows_list\n",
    "            flows_list.append(flow)\n",
    "    # Return dictionary of IPs and flows\n",
    "    return {ip: flows_list}\n",
    "\n",
    "def process_flow(flow):    \n",
    "    # Create hash of protocol\n",
    "    proto_hash = hasher(flow[1][2])        \n",
    "    # Create hash of SrcIP\n",
    "    srcip_hash = hasher(flow[1][3])        \n",
    "    # Create hash of Sport\n",
    "    srcprt_hash = hasher(flow[1][4]) \n",
    "    # Create hash of DstIP\n",
    "    dstip_hash = hasher(flow[1][6])    \n",
    "    # Create hash of Dport\n",
    "    dstprt_hash = hasher(flow[1][7]) \n",
    "    # Cast flow entry as list for manipulation\n",
    "    flow_list = list(flow)       \n",
    "    # Insert hashes as entry in tuple for each flow\n",
    "    flow_list.insert(4, (str(proto_hash), str(srcip_hash), str(srcprt_hash), \n",
    "                         str(dstip_hash), str(dstprt_hash)))    \n",
    "    # Re-cast flow entry as tuple w/ added hash tuple\n",
    "    flow = tuple(flow_list)\n",
    "    return(flow)\n",
    "\n",
    "def single_hash(flow):\n",
    "    flow_hash = hasher(flow)\n",
    "    flow_list = list(flow)\n",
    "    flow_list.insert(4, str(flow_hash))\n",
    "    flow = tuple(flow_list) \n",
    "    return(flow)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import netflow capture file\n",
    "\n",
    "flowdata = pd.DataFrame()\n",
    "\n",
    "cap_files = [\"capture20110810.binetflow\",\"capture20110811.binetflow\"]\n",
    "\n",
    "for f in cap_files:\n",
    "    frame = pd.read_csv(f, sep=',', header=0)\n",
    "    flowdata = flowdata.append(frame, ignore_index=True)\n",
    "\n",
    "#flowdata = pd.read_csv(\"capture20110810.binetflow\", sep=',', header=0)\n",
    "\n",
    "# Strip whitespace\n",
    "flowdata.rename(columns=lambda x: x.strip(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subsample_cats = flowdata.loc[:,['SrcAddr', 'DstAddr', 'Dport','Proto', 'Label']]\n",
    "#subsample_labels = flowdata.loc[:,['Label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec (co-occurence idea for flow data)\n",
    "\n",
    "Attempting to find some co-occurence patterns in the flow data according to how an algorithm like word2vec, in its skip-gram implementation specifically for this work, works. The idea is that flows, $V_{f}$ for vector representation, that occur within a window $W_{f}$, which can be modeled as \"time\" using timestamps from the capture or just raw capture data. For brevity we will be testing raw capture for now and will transition to \"time\" in the future. I'll transition to writing a time based function futher down in the notebook.\n",
    "\n",
    "* Note here that using time-stamp windowing will allow for variable sentence length\n",
    "\n",
    "Considering the conditional probabilities $P(w|f)$, with a given set of flow captures _Captures_, the goal is to set the parameters $\\theta$ of $P(w|f;\\theta)$ so as to maximize the capture probability :\n",
    "\n",
    "$$\\underset{\\theta}{\\operatorname{argmax}} \\underset{f \\in Captures}{\\operatorname{\\prod}} \\left[\\underset{w \\in W_{f}}{\\operatorname{\\prod}} P(w \\vert f;\\theta)\\right] $$\n",
    "\n",
    "in this equation $W_{f}$ is a set of surrounding flows of flow $f$. Alternatively :\n",
    "\n",
    "$$ \\underset{\\theta}{\\operatorname{argmax}} \\underset{(f, w) \\in D}{\\operatorname{\\prod}} P(w \\vert f;\\theta) $$\n",
    "\n",
    "Here $D$ is the set of all flow and window pairs we extract from the text.\n",
    "\n",
    "The word2vec algorithm seems to capture an underlying phenomenon of written language that clusters words together according to their linguistic similarity, this can be seen in something like simple synonym analysis. The goal is to exploit this underlying \"similarity\" phenomenon in the \"conversations\" that machines are having within a given data network. This can also be thought of as written text being an express conversation between the writer and the reader. This is the idea behind using word2vec on flow capture data.\n",
    "\n",
    "Each \"time step\", right now just being a subset of a given flow data set, is as a 'sentence' in the word2vec model. We should then be able to find flow \"similarities\" that exist within the context of flows. The idea is this \"symilarity\" will really just yield an occurence pattern over the flow data, much like word2vec does for written text.\n",
    "\n",
    "Another part of the idea is much like in written text there are word / context, $(w,c)$, patterns that are discovered and exploited when running the algorithm over a given set of written language. There are common occurences and patterns that can be yielded from flow data, much like the occurences and patterns that are mined from written text. At the end of the embedding excersize we can use k-means to attempt to cluster the flows. This should yield some sort of clustering of commonly occuring flows that have the same occurence measure in a given set of netflow captures. We can then use this data to measure against other, unseen, flows for future classification of \"anamoly\". I use that word loosely as this is strictly expirimental.\n",
    "\n",
    "### Assumptions :\n",
    "\n",
    "#### Maximizing the objective will result in good embeddings $v_{f}  \\forall w \\in V$\n",
    "\n",
    "#####_It is important to note with the above statment, with respect to time, is the assumption that the data I am operating from has already been ordered according to the tooling I used to acquire it_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram Negative Sampling\n",
    "\n",
    "One of the other portions of the word2vec algorithm that I will be testing in this experiment will be negative sampling.\n",
    "\n",
    "The objective of Skipgram with Negative Sampling is to maximize the the probability that $(f,w)$ came from the data $D$. This can be modeled as a distribution such that $P(D=1|f,w)$ be the probability that $(f,w)$ came from the data and $P(D=0|f,w) = 1 - P(D=1|f,w)$ the probability that $(f,w)$ did not. \n",
    "\n",
    "The distribution is modeled as :\n",
    "\n",
    "$$P(D=1|f,w) = \\sigma(\\vec{f} \\cdot \\vec{w}) = \\frac{1}{1+e^{-\\vec{f} \\cdot \\vec{w}}}$$\n",
    "\n",
    "where $\\vec{f}$ and $\\vec{w}$ (each a d-dimensional vector) are the model parameters to be learned.\n",
    "\n",
    "The negative sampling tries to maximize $P(D=1|f,w)$ for observed $(f,w)$ pairs while maximizing $P(D=0|f,w)$ for stochastically sampled \"negative\" examples, under the assumption that selecting a context for a given word is likely to result in an unobserved $(f,w)$ pair.\n",
    "\n",
    "SGNS's objective for a single $(f,w)$ observation is then:\n",
    "\n",
    "$$ \\log \\sigma(\\vec{f} \\cdot \\vec{w}) + k \\cdot \\mathbb{E}_{w_{N} \\sim P_{D}} [\\log \\sigma(\\vec{-f} \\cdot \\vec{w}_N)] $$\n",
    "\n",
    "where $k$ is the number of \"negative\" samples and $c_{N}$ is the sampled window, drawn according to the empirical unigram distribution $P_{D}(w) = \\frac{\\#w}{|D|}$\n",
    "\n",
    "This object is then trained in an online fashion using stochastic gradient updated over the observed pairs in the corpus $D$. The goal objective then sums over the observed $(f,w)$ pairs in the corpus :\n",
    "\n",
    "$$ \\ell = \\Sigma_{f \\in V_{f}} \\Sigma_{w \\in V_{w}} \\#(f,w)(\\log \\sigma(\\vec{f} \\cdot \\vec{w}) + k \\cdot \\mathbb{E}_{w_{N} \\sim P_{D}} [\\log \\sigma(\\vec{-f} \\cdot \\vec{w}_N)]$$\n",
    "\n",
    "Optimizing this objective groups flows that have similar embeddings, while scattering unobserved pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to slide window over dataframe of \n",
    "# flowdata and create \"sentences\"\n",
    "\n",
    "def create_corpora(dataframe, window, corpus_count):\n",
    "    corpus = []\n",
    "    corpora = []\n",
    "    begin = 0\n",
    "    end = 0\n",
    "    for i in range(corpus_count):\n",
    "        while end <= window:\n",
    "            end += 1\n",
    "        else:\n",
    "            corpus.append(dataframe[begin:(end-1)])\n",
    "        begin = begin + window\n",
    "        end = end + window\n",
    "    corpora.append(corpus)\n",
    "    return(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora = create_corpora(subsample_cats, 30, 153333)\n",
    "#labels = create_corpora(subsample_labels, 150, 18500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert all tuples created by previous create_corpora function\n",
    "# to strings for use with tokenization which is then used in the\n",
    "# word2vec algorithm below \n",
    "\n",
    "str_corpora = []\n",
    "\n",
    "for corpus in corpora[0]:\n",
    "    str_corpus = []\n",
    "    for sentence in corpus.values.tolist():\n",
    "        str_corpus.append(str(sentence).encode('utf-8'))\n",
    "    str_corpora.append(str_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we train a model without using the negative sampling hyperparameter \n",
    "# We will be using this for testing of accuracy of model vs. using the \n",
    "# negative sampling function\n",
    "\n",
    "flow_model = gensim.models.Word2Vec(str_corpora, workers=23, size=200, window=20, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we train a model using the negative sampling which we will then compare\n",
    "# to the model above for the impact that the negative sampling has on the \n",
    "# clustering of flows\n",
    "\n",
    "flow_model_1 = gensim.models.Word2Vec(str_corpora, workers=23, size=100, window=30, negative=10, sample=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary results (very rough, no real hyperparameter tunings, etc.)\n",
    "\n",
    "We can see below the results may prove to be useful with respect to certain labels present in the dataset, but not others. This may have to do with the raw occurence rates of certain flow and window #$(f,w)$ combinations vs. others. I use labels lightly as well as this will ultimately become an exercise of semi-supervised learning as it can sometimes be impossible for humans to interpret the results of an unsupervised learning task without any type of contextual insight, as labels can provide\n",
    "\n",
    "We can tune for this using subsampling above in the SGNS model. Which will we do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['147.32.84.165', '192.5.5.241', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9761667847633362),\n",
       " (\"['147.32.84.165', '202.12.27.33', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9741541743278503),\n",
       " (\"['147.32.84.165', '128.8.10.90', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.973616898059845),\n",
       " (\"['147.32.84.165', '78.47.76.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9714504480361938),\n",
       " (\"['147.32.84.165', '193.0.14.129', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9692395925521851),\n",
       " (\"['147.32.84.165', '199.7.83.42', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9687032699584961),\n",
       " (\"['147.32.84.165', '192.228.79.201', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9674479961395264),\n",
       " (\"['147.32.84.165', '192.58.128.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9664252400398254),\n",
       " (\"['147.32.84.165', '92.53.98.100', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9656703472137451),\n",
       " (\"['147.32.84.165', '192.112.36.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9654155969619751)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for flow similarity, preferrably a flow that has the botnet label\n",
    "\n",
    "flow_model_1.most_similar(\"['147.32.84.165', '192.33.4.12', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['217.66.146.105', '147.32.84.229', '443', 'tcp', 'flow=Background-TCP-Established']\",\n",
       "  0.970333993434906),\n",
       " (\"['188.26.176.163', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.963600218296051),\n",
       " (\"['114.75.11.242', '147.32.84.229', '80', 'tcp', 'flow=Background-TCP-Established']\",\n",
       "  0.9627201557159424),\n",
       " (\"['147.32.86.96', '147.32.87.29', '0xb612', 'icmp', 'flow=Background']\",\n",
       "  0.9622609615325928),\n",
       " (\"['195.234.241.9', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.9621870517730713),\n",
       " (\"['41.130.66.62', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.9606925249099731),\n",
       " (\"['131.104.149.212', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.9604771733283997),\n",
       " (\"['147.32.84.59', '90.146.27.130', '46356', 'udp', 'flow=Background-Attempt-cmpgw-CVUT']\",\n",
       "  0.9597481489181519),\n",
       " (\"['147.32.84.229', '78.141.179.11', '34046', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.9597265720367432),\n",
       " (\"['147.32.84.59', '114.40.199.143', '21323', 'udp', 'flow=Background-Established-cmpgw-CVUT']\",\n",
       "  0.9592392444610596)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_model_1.most_similar(\"['147.32.84.165', '60.190.223.75', '888', 'tcp', 'flow=From-Botnet-V42-TCP-CC6-Plain-HTTP-Encrypted-Data']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated flows, equivalent to \"phrases\"\n",
    "\n",
    "word2vec has the option of being able to model phrases of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Now that we have some vector representations of occurences of flows within the captures that we have, we can run a clustering algorithm over them to see if we can humanly identify some of the groupings that have taken place. For this, we'll use kmeans within the scikit-learn package.\n",
    "\n",
    "Kmeans has an objective function that intends to partition $n$ objects into $k$ clusters in which each object, $n$, belongs to the cluster with the nearest mean. This can be seen as :\n",
    "\n",
    "$$ J = \\sum_{j=1}^{k}\\sum_{i=1}^{n} \\| x_{i}^{(j)} - c_{j}\\|^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set k (number of clusters) to be 1/5 of the \"vocabulary\" size\n",
    "# or an average of flows per cluster, this is a hyperparameter\n",
    "# in kmeans that we can tweak later on\n",
    "\n",
    "flow_vectors = flow_model_1.syn0[0:20000]\n",
    "num_clusters = flow_vectors.shape[0] / 5\n",
    "\n",
    "# Initialize k-means object and use it to extract centroids\n",
    "\n",
    "kmeans_clustering = cluster.KMeans(n_clusters = num_clusters, init=\"k-means++\", n_jobs=-1)\n",
    "idx = kmeans_clustering.fit_predict(flow_vectors)\n",
    "\n",
    "# Create a flow / Index dictionary, mapping \"vocabulary words\" to\n",
    "# a cluster number\n",
    "\n",
    "flow_centroid_map = dict(zip(flow_model_1.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['147.32.84.165', '209.86.93.226', '25', 'tcp', 'flow=From-Botnet-V43-TCP-Attempt-SPAM']\",\n",
       "  3),\n",
       " (\"['147.32.84.165', '192.33.4.12', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  14),\n",
       " (\"['147.32.84.165', '85.214.220.206', '25', 'tcp', 'flow=From-Botnet-V42-TCP-Attempt-SPAM']\",\n",
       "  40),\n",
       " (\"['147.32.84.165', '77.88.210.88', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  48),\n",
       " (\"['147.32.84.165', '75.180.132.243', '25', 'tcp', 'flow=From-Botnet-V42-TCP-Attempt-SPAM']\",\n",
       "  49),\n",
       " (\"['147.32.84.165', '67.23.231.68', '53', 'udp', 'flow=From-Botnet-V42-UDP-Attempt-DNS']\",\n",
       "  73),\n",
       " (\"['147.32.84.165', '60.190.223.75', '888', 'tcp', 'flow=From-Botnet-V42-TCP-CC6-Plain-HTTP-Encrypted-Data']\",\n",
       "  74),\n",
       " (\"['147.32.84.165', '80.93.50.53', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  89),\n",
       " (\"['147.32.84.165', '94.100.176.20', '25', 'tcp', 'flow=From-Botnet-V43-TCP-Attempt-SPAM']\",\n",
       "  91),\n",
       " (\"['147.32.84.165', '74.125.159.27', '25', 'tcp', 'flow=From-Botnet-V42-TCP-Attempt-SPAM']\",\n",
       "  99)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "sorted_clusters = sorted(flow_centroid_map.items(), key=operator.itemgetter(1))\n",
    "\n",
    "botnets = []\n",
    "\n",
    "for i in sorted_clusters:\n",
    "    if re.search(r\"Botnet\", i[0]):\n",
    "        botnets.append(i)\n",
    "        \n",
    "botnets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['147.32.84.59', '72.21.210.129', '80', 'tcp', 'flow=Background-Established-cmpgw-CVUT']\",\n",
       "  73),\n",
       " (\"['62.162.92.225', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  73),\n",
       " (\"['147.32.84.59', '208.88.186.10', '34021', 'udp', 'flow=Background-Established-cmpgw-CVUT']\",\n",
       "  73),\n",
       " (\"['147.32.84.165', '67.23.231.68', '53', 'udp', 'flow=From-Botnet-V42-UDP-Attempt-DNS']\",\n",
       "  73),\n",
       " (\"['200.148.213.27', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  73),\n",
       " (\"['187.75.138.219', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  73)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_members = []\n",
    "for i in sorted_clusters:\n",
    "    if i[1] == 73:\n",
    "        cluster_members.append(i)\n",
    "    \n",
    "cluster_members[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Cluster visualization\n",
    "\n",
    "Raw flow vectors $V_{f}$, created by word2vec, are embedded in dimensionality equivalent to the input layer of the shallow neural network that is used within the model. In our example we're using \n",
    "\n",
    "### t-SNE Visualization\n",
    "\n",
    "Use t-SNE and matplotlib to visualize the clusters created using Word2Vec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_tsne(word_vector):\n",
    "    tsne = manifold.TSNE(n_components=2, random_state=42)\n",
    "    return tsne.fit_transform(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#flow_model_reduced = TruncatedSVD(n_components=100, random_state=42).fit_transform(flow_model_1.syn0)\n",
    "test_tsne = manifold.TSNE(n_components=2, learning_rate=50).fit_transform(flow_model_1.syn0[0:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'), figsize=(10, 10))\n",
    "\n",
    "x = test_tsne[:,0]\n",
    "y = test_tsne[:,1]\n",
    "\n",
    "mpld3_scatter = ax.scatter(x, y, cmap='Blues', c = y)\n",
    "ax.grid(color='white', linestyle='solid')\n",
    "\n",
    "labels = [v[0] for k,v in enumerate(flow_model_1.vocab.items()[0-4000:])]\n",
    "tooltip = mpld3.plugins.PointLabelTooltip(mpld3_scatter, labels=labels)\n",
    "mpld3.plugins.connect(fig, tooltip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'), figsize=(10, 10))\n",
    "\n",
    "\n",
    "mpld3_scatter = ax.scatter(tsne_objs[0][:, 0], tsne_objs[0][:, 1])\n",
    "ax.grid(color='white', linestyle='solid')\n",
    "\n",
    "#ax.set_title(\"Scatter Plot (with tooltips!)\", size=20)\n",
    "\n",
    "#labels = [v[0][0] for k,v in enumerate(sample)]\n",
    "tooltip = mpld3.plugins.PointLabelTooltip(mpld3_scatter)\n",
    "mpld3.plugins.connect(fig, tooltip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(70, 70))\n",
    "ax = plt.axes(frameon=False)\n",
    "plt.setp(ax,xticks=(), yticks=())\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.9,\n",
    "                wspace=0.0, hspace=0.0)\n",
    "plt.scatter(flow_model_embedded_1[:, 0], flow_model_embedded_1[:, 1], marker=\"x\")\n",
    "\n",
    "#for k,v in enumerate(flow_model.vocab.items()):\n",
    "#    plt.annotate(v[0], flow_model_embedded_1[k])\n",
    "\n",
    "plt.savefig('test2.eps', format='eps', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Things left to test\n",
    "\n",
    "* Running true tuples of SRCIP, DSTIP, DSTPORT, and PROTO (label included for now, need to figure out how to persist through pipeline without skewing results)\n",
    "* Tune hyperparameters of models for all algorithms (word2vec, kmeans, tSNE)\n",
    "* Find fixes for limitations of larger datasets for tooling that has dependencies on numpy (kmeans, tSNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate hash for all flows within the dataset\n",
    "\n",
    "flowdata_dict = {}\n",
    "\n",
    "# Parallelize the hashing of flows\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for flow in executor.map(process_flow, flowdata_sample.iterrows()):\n",
    "        flowdata_dict[flow[0]] = (flow[1], flow[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ONLY USE THIS BLOCK IF YOU WANT PER TO SORT FLOWS PER IP\n",
    "\n",
    "# Lists for tcp and udp flows\n",
    "\n",
    "tcp_flows = []\n",
    "udp_flows = []\n",
    "\n",
    "# Iterate over dataframe\n",
    "\n",
    "for d in flowdata_sample.iterrows():\n",
    "    if d[1][2] == 'tcp':\n",
    "        \n",
    "        #Append flow\n",
    "        \n",
    "        tcp_flows.append(d)\n",
    "        \n",
    "    elif d[1][2] == 'udp':\n",
    "        udp_flows.append(d)\n",
    "\n",
    "# Set for identifying unique IPs from flows\n",
    "\n",
    "unique_per_proto = set()\n",
    "\n",
    "for flow in tcp_flows:\n",
    "    \n",
    "    # Add unique SrcAddr to set for TCP flows\n",
    "    \n",
    "    unique_per_proto.add(flow[1][3])\n",
    "\n",
    "for flow in udp_flows:\n",
    "    \n",
    "    # Add unique SrcAddr to set for UDP flows\n",
    "    \n",
    "    unique_per_proto.add(flow[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ONLY USE THIS BLOCK IF YOU WANT PER TO SORT FLOWS PER IP\n",
    "\n",
    "# Set for unique IPs for overall flowset \n",
    "# Maintaining ordering of existing data\n",
    "# Use this if we wanted a corpus per srcIP\n",
    "\n",
    "unique_per_flow = set()\n",
    "\n",
    "for d in flowdata_sample.iterrows():\n",
    "    unique_per_flow.add(d[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort flows according to srcIP\n",
    "\n",
    "ip_dicts = []\n",
    "\n",
    "# Parallelization framework\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    \n",
    "    # pass in unique set to executor\n",
    "    # Return dict from each process\n",
    "    \n",
    "    for d in executor.map(sort_ip_flow, unique_per_proto):\n",
    "    \n",
    "        # Roll all dicts up into list\n",
    "        \n",
    "        ip_dicts.append(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

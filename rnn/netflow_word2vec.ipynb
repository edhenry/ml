{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pyhash\n",
    "import gensim\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "import concurrent.futures\n",
    "from pprint import pprint\n",
    "import random\n",
    "import mpld3\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable mpld3 for notebook\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "# Instantiate hasher object\n",
    "#hasher = pyhash.city_64()\n",
    "\n",
    "# Method to strip white test\n",
    "def strip(text):\n",
    "    return text.strip()\n",
    "\n",
    "# Method to set dataframe entries to integers\n",
    "def make_int(text):\n",
    "    return int(text.strip(''))    \n",
    "\n",
    "# Method to match IP against flow srcIP\n",
    "def sort_ip_flow(ip):\n",
    "    # List to house flows when matches\n",
    "    flows_list = []\n",
    "    # Iterate over tcp_flows list\n",
    "    for flow in tcp_flows:   \n",
    "        # Comparison logic - flow[1][3] corresponds to SrcIP in flow tuple\n",
    "        if ip == flow[1][3]:        \n",
    "            # Append match to flows_list\n",
    "            flows_list.append(flow)\n",
    "    # Return dictionary of IPs and flows\n",
    "    return {ip: flows_list}\n",
    "\n",
    "def process_flow(flow):    \n",
    "    # Create hash of protocol\n",
    "    proto_hash = hasher(flow[1][2])        \n",
    "    # Create hash of SrcIP\n",
    "    srcip_hash = hasher(flow[1][3])        \n",
    "    # Create hash of Sport\n",
    "    srcprt_hash = hasher(flow[1][4]) \n",
    "    # Create hash of DstIP\n",
    "    dstip_hash = hasher(flow[1][6])    \n",
    "    # Create hash of Dport\n",
    "    dstprt_hash = hasher(flow[1][7]) \n",
    "    # Cast flow entry as list for manipulation\n",
    "    flow_list = list(flow)       \n",
    "    # Insert hashes as entry in tuple for each flow\n",
    "    flow_list.insert(4, (str(proto_hash), str(srcip_hash), str(srcprt_hash), \n",
    "                         str(dstip_hash), str(dstprt_hash)))    \n",
    "    # Re-cast flow entry as tuple w/ added hash tuple\n",
    "    flow = tuple(flow_list)\n",
    "    return(flow)\n",
    "\n",
    "def single_hash(flow):\n",
    "    flow_hash = hasher(flow)\n",
    "    flow_list = list(flow)\n",
    "    flow_list.insert(4, str(flow_hash))\n",
    "    flow = tuple(flow_list) \n",
    "    return(flow)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import netflow capture file(s)\n",
    "\n",
    "flowdata = pd.DataFrame()\n",
    "\n",
    "cap_files = [\"capture20110810.binetflow\",\"capture20110811.binetflow\"]\n",
    "\n",
    "for f in cap_files:\n",
    "    frame = pd.read_csv(f, sep=',', header=0)\n",
    "    flowdata = flowdata.append(frame, ignore_index=True)\n",
    "\n",
    "# Strip whitespace\n",
    "flowdata.rename(columns=lambda x: x.strip(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subsample_cats = flowdata.loc[:,['Proto', 'SrcAddr', 'DstAddr', 'Dport']]\n",
    "subsample_labels = flowdata.loc[:,['Label']]\n",
    "\n",
    "subsample_cats_1 = flowdata.loc[:,['Proto', 'SrcAddr', 'DstAddr', 'Dport', 'Label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec (co-occurence idea for flow data)\n",
    "\n",
    "Attempting to find some co-occurence patterns in the flow data according to how an algorithm like word2vec, in its skip-gram implementation specifically for this work, works. The idea is that flows, $V_{f}$ for vector representation, that occur within a window $W_{f}$, which can be modeled as \"time\" using timestamps from the capture. A visual representation of a single flow and window of flows can be seen below :\n",
    "\n",
    "<img src=\"flow_window_5.jpg\">\n",
    "\n",
    "The image doesn't display the timestamp, but the windowing will be done according to a selected time window.\n",
    "\n",
    "Considering the conditional probabilities $P(w|f)$, with a given set of flow captures _Captures_, the goal is to set the parameters $\\theta$ of $P(w|f;\\theta)$ so as to maximize the capture probability :\n",
    "\n",
    "$$\\underset{\\theta}{\\operatorname{argmax}} \\underset{f \\in Captures}{\\operatorname{\\prod}} \\left[\\underset{w \\in W_{f}}{\\operatorname{\\prod}} P(w \\vert f;\\theta)\\right] $$\n",
    "\n",
    "in this equation $W_{f}$ is a set of surrounding flows of flow $f$. Alternatively :\n",
    "\n",
    "$$ \\underset{\\theta}{\\operatorname{argmax}} \\underset{(f, w) \\in D}{\\operatorname{\\prod}} P(w \\vert f;\\theta) $$\n",
    "\n",
    "Here $D$ is the set of all flow and window pairs we extract from the text.\n",
    "\n",
    "The word2vec algorithm seems to capture an underlying phenomenon of written language that clusters words together according to their linguistic similarity, this can be seen in something like simple synonym analysis. The goal is to exploit this underlying \"similarity\" phenomenon with respect to co-occurence of flows in a given flow capture.\n",
    "\n",
    "Each \"time step\", right now just being a subset of a given flow data set, is as a 'sentence' in the word2vec model. We should then be able to find flow \"similarities\" that exist within the context of flows. The idea is this \"symilarity\" will really just yield an occurence pattern over the flow data, much like word2vec does for written text.\n",
    "\n",
    "Another part of the idea is much like in written text there are word / context, $(w,c)$, patterns that are discovered and exploited when running the algorithm over a given set of written language. There are common occurences and patterns that can be yielded from flow data, much like the occurences and patterns that are yielded from written text.\n",
    "\n",
    "At the end of the embedding exercise we can use k-means to attempt to cluster flows, according to the embedding vectors that are produced through the word2vec algorithm. This should yield some sort of clustering of commonly occuring flows that have the same occurence measure in a given set of netflow captures. We can then use this data to measure against other, unseen, flows for future classification of \"anamoly\". I use that word loosely as this is strictly expirimental.\n",
    "\n",
    "### Assumptions :\n",
    "\n",
    "#### Maximizing the objective will result in good embeddings $v_{f}  \\forall w \\in V$\n",
    "\n",
    "#####_It is important to note with the above statment, with respect to time, is the assumption that the data I am operating from has already been ordered according to the tooling I used to acquire it_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram Negative Sampling\n",
    "\n",
    "One of the other portions of the word2vec algorithm that I will be testing in this experiment will be negative sampling.\n",
    "\n",
    "The objective of Skipgram with Negative Sampling is to maximize the the probability that $(f,w)$ came from the data $D$. This can be modeled as a distribution such that $P(D=1|f,w)$ be the probability that $(f,w)$ came from the data and $P(D=0|f,w) = 1 - P(D=1|f,w)$ the probability that $(f,w)$ did not. \n",
    "\n",
    "The distribution is modeled as :\n",
    "\n",
    "$$P(D=1|f,w) = \\sigma(\\vec{f} \\cdot \\vec{w}) = \\frac{1}{1+e^{-\\vec{f} \\cdot \\vec{w}}}$$\n",
    "\n",
    "where $\\vec{f}$ and $\\vec{w}$ (each a d-dimensional vector) are the model parameters to be learned.\n",
    "\n",
    "The negative sampling tries to maximize $P(D=1|f,w)$ for observed $(f,w)$ pairs while maximizing $P(D=0|f,w)$ for stochastically sampled \"negative\" examples, under the assumption that selecting a context for a given word is likely to result in an unobserved $(f,w)$ pair.\n",
    "\n",
    "SGNS's objective for a single $(f,w)$ output observation is then:\n",
    "\n",
    "$$ E = \\log \\sigma(\\vec{f} \\cdot \\vec{w}) + k \\cdot \\mathbb{E}_{w_{N} \\sim P_{D}} [\\log \\sigma(\\vec{-f} \\cdot \\vec{w}_N)] $$\n",
    "\n",
    "where $k$ is the number of \"negative\" samples and $w_{N}$ is the sampled window, drawn according to the empirical unigram distribution $P_{D}(w) = \\frac{\\#w}{|D|}$\n",
    "\n",
    "Let's disassemble this objective function into its respective terms and put it back together again :\n",
    "\n",
    "The term $\\log \\sigma(\\vec{f} \\cdot \\vec{w})$, from above, is used to model the \n",
    "\n",
    "This object is then trained in an online fashion using stochastic gradient updated over the observed pairs in the corpus $D$. The goal objective then sums over the observed $(f,w)$ pairs in the corpus :\n",
    "\n",
    "$$ \\ell = \\Sigma_{f \\in V_{f}} \\Sigma_{w \\in V_{w}} \\#(f,w)(\\log \\sigma(\\vec{f} \\cdot \\vec{w}) + k \\cdot \\mathbb{E}_{w_{N} \\sim P_{D}} [\\log \\sigma(\\vec{-f} \\cdot \\vec{w}_N)]$$\n",
    "\n",
    "Optimizing this objective groups flows that have similar embeddings, while scattering unobserved pairs.\n",
    "\n",
    "####TODO (further exploration) : \n",
    "\n",
    "* Running true tuples of SRCIP, DSTIP, DSTPORT, and PROTO (label included for now, need to figure out how to persist through pipeline without skewing results - need to figure out how to match up labeling to flow after word2vec has been run)\n",
    "* Implement timestamp window oriented 'sentence' creation, current implementation created same length flow 'sentences' for every $f$ flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to slide window over dataframe of \n",
    "# flowdata and create \"sentences\"\n",
    "\n",
    "def create_corpora(dataframe, window, corpus_count):\n",
    "    corpus = []\n",
    "    corpora = []\n",
    "    begin = 0\n",
    "    end = 0\n",
    "    for i in range(corpus_count):\n",
    "        while end <= window:\n",
    "            end += 1\n",
    "        else:\n",
    "            corpus.append(dataframe[begin:(end-1)])\n",
    "        begin = begin + window\n",
    "        end = end + window\n",
    "    corpora.append(corpus)\n",
    "    return(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora = create_corpora(subsample_cats, 30, 153333)\n",
    "labels = create_corpora(subsample_labels, 30, 153333)\n",
    "corpora_1 = create_corpora(subsample_cats_1, 30, 153333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['94.44.127.113', '147.32.84.59', '6881', 'tcp']\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all tuples created by previous create_corpora function\n",
    "# to strings for use with tokenization which is then used in the\n",
    "# word2vec algorithm below \n",
    "\n",
    "str_corpora = []\n",
    "\n",
    "for corpus in corpora[0]:\n",
    "    str_corpus = []\n",
    "    for sentence in corpus.values.tolist():\n",
    "        str_corpus.append(str(sentence).encode('utf-8'))\n",
    "    str_corpora.append(str_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we train a model without using the negative sampling hyperparameter \n",
    "# We will be using this for testing of accuracy of model vs. using the \n",
    "# negative sampling function\n",
    "\n",
    "flow_model = gensim.models.Word2Vec(str_corpora, workers=23, size=200, window=20, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we train a model using the negative sampling which we will then compare\n",
    "# to the model above for the impact that the negative sampling has on the \n",
    "# clustering of flows\n",
    "\n",
    "flow_model_sgns = gensim.models.Word2Vec(str_corpora, workers=23, size=100, window=30, negative=10, sample=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary results (very rough, no real hyperparameter tunings / exploration, etc.)\n",
    "\n",
    "We can see below the results may prove to be useful with respect to certain labels present in the dataset, but not others. This may have to do with the raw occurence rates of certain flow and window #$(f,w)$ combinations vs. others. I use labels lightly as well as this will ultimately become an exercise of semi-supervised learning as it can sometimes be impossible for humans to interpret the results of an unsupervised learning task without any type of contextual insight, as labels can provide. In the case of written language, the \"insight\" that is provided is the fact that we know what the meanings of words are within the language and if they're clustering correctly, re: synonyms and antonyms, etc.\n",
    "\n",
    "We can tune for this using subsampling above in the SGNS model. Which will we do next.\n",
    "\n",
    "####TODO:\n",
    "* GridSearch for hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that there is indeed a clustering that has happened with respect to the \"From-Botnet-V42-UDP-DNS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['147.32.84.165', '192.5.5.241', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9761667847633362),\n",
       " (\"['147.32.84.165', '202.12.27.33', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9741541743278503),\n",
       " (\"['147.32.84.165', '128.8.10.90', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.973616898059845),\n",
       " (\"['147.32.84.165', '78.47.76.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9714504480361938),\n",
       " (\"['147.32.84.165', '193.0.14.129', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9692395925521851),\n",
       " (\"['147.32.84.165', '199.7.83.42', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9687032699584961),\n",
       " (\"['147.32.84.165', '192.228.79.201', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9674479961395264),\n",
       " (\"['147.32.84.165', '192.58.128.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9664252400398254),\n",
       " (\"['147.32.84.165', '92.53.98.100', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9656703472137451),\n",
       " (\"['147.32.84.165', '192.112.36.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9654155969619751),\n",
       " (\"['147.32.84.165', '198.41.0.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9644977450370789),\n",
       " (\"['147.32.84.165', '192.203.230.10', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9633801579475403),\n",
       " (\"['147.32.84.165', '192.36.148.17', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9618400931358337),\n",
       " (\"['147.32.84.165', '128.63.2.53', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.958657443523407),\n",
       " (\"['147.32.84.165', '89.108.64.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9581812024116516),\n",
       " (\"['147.32.84.165', '82.103.128.82', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9558319449424744),\n",
       " (\"['147.32.84.165', '192.42.93.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9557339549064636),\n",
       " (\"['147.32.84.165', '192.26.92.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9556182026863098),\n",
       " (\"['147.32.84.165', '194.226.96.8', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9543852210044861),\n",
       " (\"['147.32.84.165', '194.85.61.20', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.953228771686554),\n",
       " (\"['147.32.84.165', '88.212.196.130', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9526883959770203),\n",
       " (\"['147.32.84.165', '195.128.49.14', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9500119090080261),\n",
       " (\"['147.32.84.165', '217.16.20.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9483109712600708),\n",
       " (\"['147.32.84.165', '85.10.210.157', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9481122493743896),\n",
       " (\"['147.32.84.165', '92.53.116.200', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9478355050086975),\n",
       " (\"['147.32.84.165', '88.212.221.11', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9470769166946411),\n",
       " (\"['147.32.84.165', '82.146.55.155', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9461535811424255),\n",
       " (\"['147.32.84.165', '192.41.162.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9459192156791687),\n",
       " (\"['147.32.84.165', '77.222.40.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9456772804260254),\n",
       " (\"['147.32.84.165', '199.19.57.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.945094645023346),\n",
       " (\"['147.32.84.165', '89.253.192.21', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9428556561470032),\n",
       " (\"['147.32.84.165', '199.249.120.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9426734447479248),\n",
       " (\"['147.32.84.165', '192.54.112.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9423930048942566),\n",
       " (\"['147.32.84.165', '195.2.83.38', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9414822459220886),\n",
       " (\"['147.32.84.165', '89.108.104.3', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9414548873901367),\n",
       " (\"['147.32.84.165', '78.108.89.252', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9414442181587219),\n",
       " (\"['147.32.84.165', '80.93.50.53', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9408544898033142),\n",
       " (\"['147.32.84.165', '192.31.80.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9401237368583679),\n",
       " (\"['147.32.84.165', '195.161.112.91', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.939973771572113),\n",
       " (\"['147.32.84.165', '193.169.178.59', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9395020008087158),\n",
       " (\"['147.32.84.165', '192.48.79.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9393561482429504),\n",
       " (\"['147.32.84.165', '192.33.14.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9386749267578125),\n",
       " (\"['147.32.84.165', '85.10.210.144', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9382632970809937),\n",
       " (\"['147.32.84.165', '192.12.94.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9372074007987976),\n",
       " (\"['147.32.84.165', '192.35.51.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9371063113212585),\n",
       " (\"['147.32.84.165', '213.177.97.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9366581439971924),\n",
       " (\"['147.32.84.165', '95.163.69.51', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9363342523574829),\n",
       " (\"['147.32.84.165', '79.174.72.215', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.936087965965271),\n",
       " (\"['147.32.84.165', '195.248.235.219', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9358514547348022),\n",
       " (\"['147.32.84.165', '217.16.16.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9352473020553589),\n",
       " (\"['147.32.84.165', '78.108.81.247', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9348022937774658),\n",
       " (\"['147.32.84.165', '192.5.6.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.934520423412323),\n",
       " (\"['147.32.84.165', '199.19.56.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.934291422367096),\n",
       " (\"['147.32.84.165', '217.16.22.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9341065883636475),\n",
       " (\"['147.32.84.165', '192.36.144.107', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9333975315093994),\n",
       " (\"['147.32.84.165', '81.177.24.54', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9332102537155151),\n",
       " (\"['147.32.84.165', '192.52.178.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9328247308731079),\n",
       " (\"['147.32.84.165', '83.222.0.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9324507117271423),\n",
       " (\"['147.32.84.165', '95.168.160.245', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9320420026779175),\n",
       " (\"['147.32.84.165', '95.168.174.25', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9319052696228027),\n",
       " (\"['147.32.84.165', '80.93.56.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9313104748725891),\n",
       " (\"['147.32.84.165', '193.227.240.37', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9309282302856445),\n",
       " (\"['147.32.84.165', '208.100.5.254', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9303311705589294),\n",
       " (\"['147.32.84.165', '77.221.130.250', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9299085140228271),\n",
       " (\"['147.32.84.165', '192.55.83.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9297108054161072),\n",
       " (\"['147.32.84.165', '84.252.138.21', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9296650886535645),\n",
       " (\"['147.32.84.165', '192.43.172.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.928945779800415),\n",
       " (\"['147.32.84.165', '89.111.177.253', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9288318753242493),\n",
       " (\"['147.32.84.165', '195.2.64.38', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9286403059959412),\n",
       " (\"['147.32.84.165', '195.128.50.221', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9278726577758789),\n",
       " (\"['147.32.84.165', '178.218.208.130', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9271195530891418),\n",
       " (\"['147.32.84.165', '192.36.125.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9268661141395569),\n",
       " (\"['147.32.84.165', '199.19.54.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9267032146453857),\n",
       " (\"['147.32.84.165', '79.137.226.102', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9260225296020508),\n",
       " (\"['147.32.84.165', '193.232.130.14', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9259271621704102),\n",
       " (\"['147.32.84.165', '193.232.142.17', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9246711730957031),\n",
       " (\"['147.32.84.165', '78.47.139.101', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.924452006816864),\n",
       " (\"['147.32.84.165', '217.174.106.66', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9236003756523132),\n",
       " (\"['147.32.84.165', '77.222.41.3', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9235631823539734),\n",
       " (\"['147.32.84.165', '83.222.1.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9203209280967712),\n",
       " (\"['147.32.84.165', '91.217.21.170', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9194321632385254),\n",
       " (\"['147.32.84.165', '89.108.122.149', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.919041633605957),\n",
       " (\"['147.32.84.165', '91.217.20.170', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9166457056999207),\n",
       " (\"['147.32.84.165', '193.227.240.38', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9165226221084595),\n",
       " (\"['147.32.84.165', '78.108.80.90', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9164752960205078),\n",
       " (\"['147.32.84.165', '78.110.50.60', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.915980875492096),\n",
       " (\"['147.32.84.165', '178.162.177.145', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9158682227134705),\n",
       " (\"['147.32.84.165', '194.85.252.62', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.915434718132019),\n",
       " (\"['147.32.84.165', '77.221.159.237', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9152796864509583),\n",
       " (\"['147.32.84.165', '193.232.146.170', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9140732884407043),\n",
       " (\"['147.32.84.165', '199.249.112.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9137414693832397),\n",
       " (\"['147.32.84.165', '87.224.128.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9121522307395935),\n",
       " (\"['147.32.84.165', '93.170.25.253', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9113649725914001),\n",
       " (\"['147.32.84.165', '195.209.63.181', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9110352993011475),\n",
       " (\"['147.32.84.165', '195.243.137.26', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9104222059249878),\n",
       " (\"['147.32.84.165', '194.0.0.53', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9094029068946838),\n",
       " (\"['147.32.84.165', '91.218.228.18', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9092046022415161),\n",
       " (\"['147.32.84.165', '194.85.105.17', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9091553092002869),\n",
       " (\"['147.32.84.165', '193.232.156.17', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9083877801895142),\n",
       " (\"['147.32.84.165', '212.176.27.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  0.9074791669845581)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for flow similarity, preferrably a flow that has the botnet label\n",
    "\n",
    "flow_model_1.most_similar(\"['147.32.84.165', '192.33.4.12', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\", topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['217.66.146.105', '147.32.84.229', '443', 'tcp', 'flow=Background-TCP-Established']\",\n",
       "  0.970333993434906),\n",
       " (\"['188.26.176.163', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.963600218296051),\n",
       " (\"['114.75.11.242', '147.32.84.229', '80', 'tcp', 'flow=Background-TCP-Established']\",\n",
       "  0.9627201557159424),\n",
       " (\"['147.32.86.96', '147.32.87.29', '0xb612', 'icmp', 'flow=Background']\",\n",
       "  0.9622609615325928),\n",
       " (\"['195.234.241.9', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.9621870517730713),\n",
       " (\"['41.130.66.62', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.9606925249099731),\n",
       " (\"['131.104.149.212', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.9604771733283997),\n",
       " (\"['147.32.84.59', '90.146.27.130', '46356', 'udp', 'flow=Background-Attempt-cmpgw-CVUT']\",\n",
       "  0.9597481489181519),\n",
       " (\"['147.32.84.229', '78.141.179.11', '34046', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  0.9597265720367432),\n",
       " (\"['147.32.84.59', '114.40.199.143', '21323', 'udp', 'flow=Background-Established-cmpgw-CVUT']\",\n",
       "  0.9592392444610596)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_model_1.most_similar(\"['147.32.84.165', '60.190.223.75', '888', 'tcp', 'flow=From-Botnet-V42-TCP-CC6-Plain-HTTP-Encrypted-Data']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without label contained in the dataset\n",
    "\n",
    "Here we run the same hyperparameters for the word2vec algorith, this time ignoring the label and not adding it to the \"word\" representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flow_model_2 = gensim.models.Word2Vec(str_corpora, workers=23, size=100, window=30, negative=10, sample=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['147.32.84.165', '192.112.36.4', '53', 'udp']\", 0.9759483337402344),\n",
       " (\"['147.32.84.165', '193.0.14.129', '53', 'udp']\", 0.9724588394165039),\n",
       " (\"['147.32.84.165', '192.5.5.241', '53', 'udp']\", 0.9721120595932007),\n",
       " (\"['147.32.84.165', '128.8.10.90', '53', 'udp']\", 0.9712154865264893),\n",
       " (\"['147.32.84.165', '192.58.128.30', '53', 'udp']\", 0.9697802662849426),\n",
       " (\"['147.32.84.165', '192.36.148.17', '53', 'udp']\", 0.9674890041351318),\n",
       " (\"['147.32.84.165', '198.41.0.4', '53', 'udp']\", 0.9672064185142517),\n",
       " (\"['147.32.84.165', '199.7.83.42', '53', 'udp']\", 0.9657577872276306),\n",
       " (\"['147.32.84.165', '202.12.27.33', '53', 'udp']\", 0.9610617160797119),\n",
       " (\"['147.32.84.165', '192.203.230.10', '53', 'udp']\", 0.9608649015426636)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_model_2.most_similar(\"['147.32.84.165', '192.33.4.12', '53', 'udp']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartTime</th>\n",
       "      <th>Dur</th>\n",
       "      <th>Proto</th>\n",
       "      <th>SrcAddr</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Dir</th>\n",
       "      <th>DstAddr</th>\n",
       "      <th>Dport</th>\n",
       "      <th>State</th>\n",
       "      <th>sTos</th>\n",
       "      <th>dTos</th>\n",
       "      <th>TotPkts</th>\n",
       "      <th>TotBytes</th>\n",
       "      <th>SrcBytes</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1264477</th>\n",
       "      <td>2011/08/10 12:29:05.687373</td>\n",
       "      <td>0.258197</td>\n",
       "      <td>udp</td>\n",
       "      <td>147.32.84.165</td>\n",
       "      <td>2077</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>192.112.36.4</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>528</td>\n",
       "      <td>68</td>\n",
       "      <td>flow=From-Botnet-V42-UDP-DNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264673</th>\n",
       "      <td>2011/08/10 12:29:06.093217</td>\n",
       "      <td>0.258987</td>\n",
       "      <td>udp</td>\n",
       "      <td>147.32.84.165</td>\n",
       "      <td>2077</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>192.112.36.4</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>611</td>\n",
       "      <td>77</td>\n",
       "      <td>flow=From-Botnet-V42-UDP-DNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          StartTime       Dur Proto        SrcAddr Sport  \\\n",
       "1264477  2011/08/10 12:29:05.687373  0.258197   udp  147.32.84.165  2077   \n",
       "1264673  2011/08/10 12:29:06.093217  0.258987   udp  147.32.84.165  2077   \n",
       "\n",
       "           Dir       DstAddr Dport State  sTos  dTos  TotPkts  TotBytes  \\\n",
       "1264477    <->  192.112.36.4    53   CON     0     0        2       528   \n",
       "1264673    <->  192.112.36.4    53   CON     0     0        2       611   \n",
       "\n",
       "         SrcBytes                         Label  \n",
       "1264477        68  flow=From-Botnet-V42-UDP-DNS  \n",
       "1264673        77  flow=From-Botnet-V42-UDP-DNS  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowdata[flowdata['DstAddr'].str.contains(\"192.112.36.4\", na=False)].head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_flow = []\n",
    "\n",
    "for flow in flow_model_2.vocab.items():\n",
    "    if re.search(r\"192.112.36.4\", flow[0]):\n",
    "        vocab_flow.append(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['147.32.84.165', '192.112.36.4', '53', 'udp']\",\n",
       "  <gensim.models.word2vec.Vocab at 0x7f808752a350>)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flow_model_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0a3a77a56ce0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflow_model_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"['147.32.87.49', '192.5.5.241', '53', 'udp']\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'flow_model_2' is not defined"
     ]
    }
   ],
   "source": [
    "flow_model_2.most_similar(\"['147.32.87.49', '192.5.5.241', '53', 'udp']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartTime</th>\n",
       "      <th>Dur</th>\n",
       "      <th>Proto</th>\n",
       "      <th>SrcAddr</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Dir</th>\n",
       "      <th>DstAddr</th>\n",
       "      <th>Dport</th>\n",
       "      <th>State</th>\n",
       "      <th>sTos</th>\n",
       "      <th>dTos</th>\n",
       "      <th>TotPkts</th>\n",
       "      <th>TotBytes</th>\n",
       "      <th>SrcBytes</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401163</th>\n",
       "      <td>2011/08/10 10:24:11.577652</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>udp</td>\n",
       "      <td>147.32.87.49</td>\n",
       "      <td>65174</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>192.5.5.241</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>636</td>\n",
       "      <td>132</td>\n",
       "      <td>flow=Background-UDP-Established</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265468</th>\n",
       "      <td>2011/08/10 12:29:12.214663</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>udp</td>\n",
       "      <td>147.32.84.165</td>\n",
       "      <td>2077</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>192.5.5.241</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>417</td>\n",
       "      <td>70</td>\n",
       "      <td>flow=From-Botnet-V42-UDP-DNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          StartTime       Dur Proto        SrcAddr  Sport  \\\n",
       "401163   2011/08/10 10:24:11.577652  0.004420   udp   147.32.87.49  65174   \n",
       "1265468  2011/08/10 12:29:12.214663  0.002282   udp  147.32.84.165   2077   \n",
       "\n",
       "           Dir      DstAddr Dport State  sTos  dTos  TotPkts  TotBytes  \\\n",
       "401163     <->  192.5.5.241    53   CON     0     0        2       636   \n",
       "1265468    <->  192.5.5.241    53   CON     0     0        2       417   \n",
       "\n",
       "         SrcBytes                            Label  \n",
       "401163        132  flow=Background-UDP-Established  \n",
       "1265468        70     flow=From-Botnet-V42-UDP-DNS  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowdata[flowdata['DstAddr'].str.contains(\"192.5.5.241\", na=False)].head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated flows, equivalent to \"phrases\"\n",
    "\n",
    "The word2vec algorithm can also learn embeddings for phrases as well as single words for written language. The ideas I have surrounding \"phrases\" would be learning the embeddings for given windows of flows, if they were to present themselves in certain capacities within the captures flow data.\n",
    "\n",
    "The current flow data that this notebook is based around are aggregated flows for bi-directional communication between endpoints. Exploiting something like capturing the 'phrase' of a flow, or thought another way, the bi-directional communication patterns that are contained within flow data might prove useful for application profiling, etc. through the use of application meta-data tracked through some sort of semi-supervised learning pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Now that we have some vector representations of occurences of flows within the captures that we have, we can run a clustering algorithm over them to see if we can humanly identify some of the groupings that have taken place. For this, we'll use kmeans within the scikit-learn package.\n",
    "\n",
    "Kmeans has an objective function that intends to partition $n$ objects into $k$ clusters in which each object, $n$, belongs to the cluster with the nearest mean. This can be seen as :\n",
    "\n",
    "$$ J = \\sum_{j=1}^{k}\\sum_{i=1}^{n} \\| x_{i}^{(j)} - c_{j}\\|^2 $$\n",
    "\n",
    "Where $\\| x_{i}^{(j)} - c_{j}\\|^2$ is a chosen distance measure between a datapoint $x^{j}_{i}$ and the cluster center $c{j}$, is an indicator of the distance of the $n$ datapoints from their respective cluster $k$ centers. In this case, $k$ is a hyperparameter that can be used within the model to define how many cluster centroids should be trained over.\n",
    "\n",
    "####TODO :\n",
    "\n",
    "* Limitation for arrays larger than 16GB due to an underlying dependency that numpy has, need to investigate - this is why I'm only running kmeans on a subset of the overall model learned above\n",
    "* Dimensionality reduction of some kind over the data - 300 dimensional data isn't crazy high but might have some improved performance here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set k (number of clusters) to be 1/5 of the \"vocabulary\" size\n",
    "# or an average of flows per cluster, this is a hyperparameter\n",
    "# in kmeans that we can tweak later on\n",
    "\n",
    "flow_vectors = flow_model_1.syn0[0:20000]\n",
    "num_clusters = flow_vectors.shape[0] / 5\n",
    "\n",
    "# Initialize k-means object and use it to extract centroids\n",
    "\n",
    "kmeans_clustering = cluster.KMeans(n_clusters = num_clusters, init=\"k-means++\", n_jobs=-1)\n",
    "idx = kmeans_clustering.fit_predict(flow_vectors)\n",
    "\n",
    "# Create a flow / Index dictionary, mapping \"vocabulary words\" to\n",
    "# a cluster number\n",
    "\n",
    "flow_centroid_map = dict(zip(flow_model_1.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['147.32.84.165', '209.86.93.226', '25', 'tcp', 'flow=From-Botnet-V43-TCP-Attempt-SPAM']\",\n",
       "  3),\n",
       " (\"['147.32.84.165', '192.33.4.12', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  14),\n",
       " (\"['147.32.84.165', '85.214.220.206', '25', 'tcp', 'flow=From-Botnet-V42-TCP-Attempt-SPAM']\",\n",
       "  40),\n",
       " (\"['147.32.84.165', '77.88.210.88', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  48),\n",
       " (\"['147.32.84.165', '75.180.132.243', '25', 'tcp', 'flow=From-Botnet-V42-TCP-Attempt-SPAM']\",\n",
       "  49),\n",
       " (\"['147.32.84.165', '67.23.231.68', '53', 'udp', 'flow=From-Botnet-V42-UDP-Attempt-DNS']\",\n",
       "  73),\n",
       " (\"['147.32.84.165', '60.190.223.75', '888', 'tcp', 'flow=From-Botnet-V42-TCP-CC6-Plain-HTTP-Encrypted-Data']\",\n",
       "  74),\n",
       " (\"['147.32.84.165', '80.93.50.53', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n",
       "  89),\n",
       " (\"['147.32.84.165', '94.100.176.20', '25', 'tcp', 'flow=From-Botnet-V43-TCP-Attempt-SPAM']\",\n",
       "  91),\n",
       " (\"['147.32.84.165', '74.125.159.27', '25', 'tcp', 'flow=From-Botnet-V42-TCP-Attempt-SPAM']\",\n",
       "  99)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find some botnet labels to use for exploration of data\n",
    "\n",
    "import operator\n",
    "sorted_clusters = sorted(flow_centroid_map.items(), key=operator.itemgetter(1))\n",
    "\n",
    "botnets = []\n",
    "\n",
    "for i in sorted_clusters:\n",
    "    if re.search(r\"Botnet\", i[0]):\n",
    "        botnets.append(i)\n",
    "        \n",
    "botnets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['147.32.84.59', '72.21.210.129', '80', 'tcp', 'flow=Background-Established-cmpgw-CVUT']\",\n",
       "  73),\n",
       " (\"['62.162.92.225', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  73),\n",
       " (\"['147.32.84.59', '208.88.186.10', '34021', 'udp', 'flow=Background-Established-cmpgw-CVUT']\",\n",
       "  73),\n",
       " (\"['147.32.84.165', '67.23.231.68', '53', 'udp', 'flow=From-Botnet-V42-UDP-Attempt-DNS']\",\n",
       "  73),\n",
       " (\"['200.148.213.27', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  73),\n",
       " (\"['187.75.138.219', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n",
       "  73)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at members of clusters according to botnet memberships discovered above\n",
    "\n",
    "cluster_members = []\n",
    "for i in sorted_clusters:\n",
    "    if i[1] == 73:\n",
    "        cluster_members.append(i)\n",
    "    \n",
    "cluster_members[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Cluster visualization\n",
    "\n",
    "Raw flow vectors $V_{f}$, created by word2vec, are embedded in dimensionality equivalent to the input layer of the shallow neural network that is used within the model. In our example we're using \n",
    "\n",
    "### t-SNE Visualization\n",
    "\n",
    "Use t-SNE and matplotlib to visualize the clusters created using Word2Vec.\n",
    "\n",
    "####TODO :\n",
    "\n",
    "* Brief explanation of the tSNE algorithm and how it handles compressing higher dimensional data into 2 or 3 dimension for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_tsne(word_vector):\n",
    "    tsne = manifold.TSNE(n_components=2, random_state=42)\n",
    "    return tsne.fit_transform(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#flow_model_reduced = TruncatedSVD(n_components=100, random_state=42).fit_transform(flow_model_1.syn0)\n",
    "test_tsne = manifold.TSNE(n_components=2, learning_rate=50).fit_transform(flow_model_1.syn0[0:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'), figsize=(10, 10))\n",
    "\n",
    "x = test_tsne[:,0]\n",
    "y = test_tsne[:,1]\n",
    "\n",
    "mpld3_scatter = ax.scatter(x, y, cmap='Blues', c = y)\n",
    "ax.grid(color='white', linestyle='solid')\n",
    "\n",
    "labels = [v[0] for k,v in enumerate(flow_model_1.vocab.items()[0-4000:])]\n",
    "tooltip = mpld3.plugins.PointLabelTooltip(mpld3_scatter, labels=labels)\n",
    "mpld3.plugins.connect(fig, tooltip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'), figsize=(10, 10))\n",
    "\n",
    "\n",
    "mpld3_scatter = ax.scatter(tsne_objs[0][:, 0], tsne_objs[0][:, 1])\n",
    "ax.grid(color='white', linestyle='solid')\n",
    "\n",
    "#ax.set_title(\"Scatter Plot (with tooltips!)\", size=20)\n",
    "\n",
    "#labels = [v[0][0] for k,v in enumerate(sample)]\n",
    "tooltip = mpld3.plugins.PointLabelTooltip(mpld3_scatter)\n",
    "mpld3.plugins.connect(fig, tooltip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(70, 70))\n",
    "ax = plt.axes(frameon=False)\n",
    "plt.setp(ax,xticks=(), yticks=())\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.9,\n",
    "                wspace=0.0, hspace=0.0)\n",
    "plt.scatter(flow_model_embedded_1[:, 0], flow_model_embedded_1[:, 1], marker=\"x\")\n",
    "\n",
    "#for k,v in enumerate(flow_model.vocab.items()):\n",
    "#    plt.annotate(v[0], flow_model_embedded_1[k])\n",
    "\n",
    "plt.savefig('test2.eps', format='eps', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Things left to research / validate / test\n",
    "\n",
    "\n",
    "* Tune hyperparameters of models for all algorithms (word2vec, kmeans, tSNE)\n",
    "* Find fixes for limitations of larger datasets for tooling that has dependencies on numpy (kmeans, tSNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- EVERYTHING BELOW THIS LINE FOR FUTURE USE ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate hash for all flows within the dataset\n",
    "\n",
    "flowdata_dict = {}\n",
    "\n",
    "# Parallelize the hashing of flows\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for flow in executor.map(process_flow, flowdata_sample.iterrows()):\n",
    "        flowdata_dict[flow[0]] = (flow[1], flow[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ONLY USE THIS BLOCK IF YOU WANT PER TO SORT FLOWS PER IP\n",
    "\n",
    "# Lists for tcp and udp flows\n",
    "\n",
    "tcp_flows = []\n",
    "udp_flows = []\n",
    "\n",
    "# Iterate over dataframe\n",
    "\n",
    "for d in flowdata_sample.iterrows():\n",
    "    if d[1][2] == 'tcp':\n",
    "        \n",
    "        #Append flow\n",
    "        \n",
    "        tcp_flows.append(d)\n",
    "        \n",
    "    elif d[1][2] == 'udp':\n",
    "        udp_flows.append(d)\n",
    "\n",
    "# Set for identifying unique IPs from flows\n",
    "\n",
    "unique_per_proto = set()\n",
    "\n",
    "for flow in tcp_flows:\n",
    "    \n",
    "    # Add unique SrcAddr to set for TCP flows\n",
    "    \n",
    "    unique_per_proto.add(flow[1][3])\n",
    "\n",
    "for flow in udp_flows:\n",
    "    \n",
    "    # Add unique SrcAddr to set for UDP flows\n",
    "    \n",
    "    unique_per_proto.add(flow[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ONLY USE THIS BLOCK IF YOU WANT PER TO SORT FLOWS PER IP\n",
    "\n",
    "# Set for unique IPs for overall flowset \n",
    "# Maintaining ordering of existing data\n",
    "# Use this if we wanted a corpus per srcIP\n",
    "\n",
    "unique_per_flow = set()\n",
    "\n",
    "for d in flowdata_sample.iterrows():\n",
    "    unique_per_flow.add(d[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort flows according to srcIP\n",
    "\n",
    "ip_dicts = []\n",
    "\n",
    "# Parallelization framework\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    \n",
    "    # pass in unique set to executor\n",
    "    # Return dict from each process\n",
    "    \n",
    "    for d in executor.map(sort_ip_flow, unique_per_proto):\n",
    "    \n",
    "        # Roll all dicts up into list\n",
    "        \n",
    "        ip_dicts.append(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
